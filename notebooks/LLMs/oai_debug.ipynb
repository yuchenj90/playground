{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c84fe8a-5eee-43fb-a607-33a32cee8530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "00 6.02623987197876\n",
      "10 0.09828782826662064\n",
      "20 0.006855769082903862\n",
      "30 0.0016643410781398416\n",
      "40 0.0008433522889390588\n",
      "50 0.0005899668904021382\n",
      "60 0.0004798386653419584\n",
      "70 0.0004194046196062118\n",
      "80 0.0003798803372774273\n",
      "90 0.00035048392601311207\n",
      "\n",
      "Samples:\n",
      "tensor([[48, 49, 95],\n",
      "        [48, 50, 95],\n",
      "        [48, 51, 95],\n",
      "        [48, 52, 95],\n",
      "        [48, 53, 95],\n",
      "        [48, 54, 95],\n",
      "        [48, 55, 95],\n",
      "        [48, 56, 95],\n",
      "        [48, 57, 95],\n",
      "        [49, 48, 95],\n",
      "        [49, 49, 95],\n",
      "        [49, 50, 95],\n",
      "        [49, 51, 95],\n",
      "        [49, 52, 95],\n",
      "        [49, 53, 95]])\n",
      "01_____________________________________________________________\n",
      "02_____________________________________________________________\n",
      "03_____________________________________________________________\n",
      "04_____________________________________________________________\n",
      "05_____________________________________________________________\n",
      "06_____________________________________________________________\n",
      "07_____________________________________________________________\n",
      "08_____________________________________________________________\n",
      "09_____________________________________________________________\n",
      "10_____________________________________________________________\n",
      "11_____________________________________________________________\n",
      "12_____________________________________________________________\n",
      "13_____________________________________________________________\n",
      "14_____________________________________________________________\n",
      "15_____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Debugging exercise: Fixing GPT-7.\n",
    "(Basic knowledge of PyTorch and transformer models is required for this task.)\n",
    "\n",
    "It's the night right before we want to launch our latest and greatest model, GPT-7.\n",
    "But, oh no! It seems some bugs have crept in at the last minute.\n",
    "We need you to fix them in time before our big launch.\n",
    "Three sections in the model are marked, each of which contains some bugs.\n",
    "Run the training/sampling code to check whether the model is working.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens, n_heads):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(hiddens, hiddens)\n",
    "        self.query = nn.Linear(hiddens, hiddens)\n",
    "        self.value = nn.Linear(hiddens, hiddens)\n",
    "        self.out = nn.Linear(hiddens, hiddens)\n",
    "        self.n_head = n_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "\n",
    "        q = self.query(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2)\n",
    "        k = self.key(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # >>> THIS SECTION CONTAINS 2 BUGS.\n",
    "        att = torch.matmul(q, k.transpose(-2, -1))\n",
    "        att = att / math.sqrt(k.shape[-1])\n",
    "        mask = torch.tril(torch.ones(t, t, dtype=torch.bool)).view(1, 1, t, t)\n",
    "        att = torch.where(mask, att, 0) \n",
    "        att = F.softmax(att, dim=-1) # (B, n_head, t, t)\n",
    "        y = torch.matmul(att, v) \n",
    "        y = v.transpose(1, 2)\n",
    "        y = y.reshape(b, t, c)\n",
    "        y = self.out(y)\n",
    "        # <<< SECTION ENDS HERE.\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens, n_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hiddens)\n",
    "        self.ln2 = nn.LayerNorm(hiddens)\n",
    "        self.attn = CausalSelfAttention(hiddens, n_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hiddens, 4 * hiddens),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * hiddens, hiddens),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tokens=64,\n",
    "                 hiddens=64,\n",
    "                 n_heads=4,\n",
    "                 layers=4,\n",
    "                 vocab=256,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # >>> THIS SECTION CONTAINS A BUG.\n",
    "        self.layers = layers\n",
    "        self.tok_emb = nn.Embedding(vocab, hiddens)\n",
    "        self.pos_emb = nn.Parameter(torch.empty(1, n_tokens, hiddens))\n",
    "        self.blocks = nn.ModuleList([Block(hiddens, n_heads) for _ in range(layers)])\n",
    "        self.ln_f = nn.LayerNorm(hiddens)\n",
    "        self.final = nn.Linear(hiddens, vocab, bias=False)\n",
    "        # <<< SECTION ENDS HERE.\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        token_embeddings = self.tok_emb(x)\n",
    "        position_embeddings = self.pos_emb[:, :t, :]\n",
    "        x = token_embeddings + position_embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.final(x)\n",
    "\n",
    "    def loss(self, tokens):\n",
    "        # >>> THIS SECTION CONTAINS A BUG.\n",
    "        targets = tokens[:, :-1]\n",
    "        logits = self.forward(tokens[:, :-1])\n",
    "        # print(targets.shape, logits.shape, logits.reshape(-1, logits.size(-1)))\n",
    "        return F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.flatten())\n",
    "        # <<< SECTION ENDS HERE.\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Trains the model and samples from it. There are no bugs here. #\n",
    "#################################################################\n",
    "\n",
    "def get_prompt():\n",
    "    text = torch.tensor(np.array([[ord(c) for c in '{:02d}_'.format(i)] for i in range(1, 16)]))\n",
    "    return text\n",
    "\n",
    "\n",
    "def sample(gpt):\n",
    "    print()\n",
    "    print('Samples:')\n",
    "    text = get_prompt()\n",
    "    print(text)\n",
    "    for _ in range(60):\n",
    "        logits = gpt(text)[:, -1:]\n",
    "        new_tokens = torch.distributions.Categorical(F.softmax(logits / 0.8, dim=-1)).sample()\n",
    "        text = torch.cat([text, new_tokens], axis=1)\n",
    "    for t in text:\n",
    "        print(''.join([chr(c) for c in t]))\n",
    "\n",
    "\n",
    "def train():\n",
    "    data = [\n",
    "        \"09_Over every mistake\",\n",
    "        \"08_But there's no sense crying\",\n",
    "        \"07_Because we can\",\n",
    "        \"12_And the science gets done\",\n",
    "        \"04_My satisfaction\",\n",
    "        \"10_You just keep on trying\",\n",
    "        \"15_Still alive\",\n",
    "        \"01_This was a triumph\",\n",
    "        \"13_And you make a neat gun\",\n",
    "        \"11_Till you run out of cake\",\n",
    "        \"06_We do what we must\",\n",
    "        \"03_It's hard to overstate\",\n",
    "        \"14_For the people who are\",\n",
    "        \"05_Aperture Science:\",\n",
    "        \"02_I'm making a note here; 'Huge success'\",\n",
    "    ]\n",
    "    data = [[ord(c) for c in d] for d in data]\n",
    "    data = torch.tensor(np.array([np.pad(x, (0, 64 - len(x)), constant_values=ord('_')) for x in data]))\n",
    "\n",
    "    gpt = GPT()\n",
    "    optimizer = torch.optim.Adam(gpt.parameters(), lr=1e-2)\n",
    "    print('Loss:')\n",
    "    for i in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        l = gpt.loss(data)\n",
    "        if i % 10 == 0:\n",
    "            print('{:02d} {}'.format(i, l.detach().numpy()))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return gpt\n",
    "\n",
    "\n",
    "def run():\n",
    "    gpt = train()\n",
    "    sample(gpt)\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cae0e2c-c6fc-44cf-b580-5719df7caffd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "00 6.317563056945801\n",
      "10 1.0246399641036987\n",
      "20 0.6647700071334839\n",
      "30 0.35471609234809875\n",
      "40 0.11384560167789459\n",
      "50 0.057370759546756744\n",
      "60 0.04002990201115608\n",
      "70 0.0340501144528389\n",
      "80 0.0331902876496315\n",
      "90 0.03291754052042961\n",
      "\n",
      "Samples:\n",
      "01_This was a triumph__________________________________________\n",
      "02_I'm making a note here; 'Huge success'______________________\n",
      "03_It's hard to overstate______________________________________\n",
      "04_My satisfaction_____________________________________________\n",
      "05_Aperture Science:___________________________________________\n",
      "06_We do what we must__________________________________________\n",
      "07_Because we can______________________________________________\n",
      "08_But there's no sense crying_________________________________\n",
      "09_Over every mistake__________________________________________\n",
      "10_You just keep on trying_____________________________________\n",
      "11_Till you run out of cake____________________________________\n",
      "12_And the science gets done___________________________________\n",
      "13_And you make a neat gun_____________________________________\n",
      "14_For the people who are______________________________________\n",
      "15_Still alive_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Debugging exercise: Fixing GPT-7.\n",
    "(Basic knowledge of PyTorch and transformer models is required for this task.)\n",
    "\n",
    "It's the night right before we want to launch our latest and greatest model, GPT-7.\n",
    "But, oh no! It seems some bugs have crept in at the last minute.\n",
    "We need you to fix them in time before our big launch.\n",
    "Three sections in the model are marked, each of which contains some bugs.\n",
    "Run the training/sampling code to check whether the model is working.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens, n_heads):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(hiddens, hiddens)\n",
    "        self.query = nn.Linear(hiddens, hiddens)\n",
    "        self.value = nn.Linear(hiddens, hiddens)\n",
    "        self.out = nn.Linear(hiddens, hiddens)\n",
    "        self.n_head = n_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size() # b=15, t=63, c=64, n_head=4\n",
    "        # print(b,t,c, self.n_head)\n",
    "\n",
    "        q = self.query(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2)\n",
    "        k = self.key(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # >>> THIS SECTION CONTAINS 2 BUGS.\n",
    "        att = torch.matmul(q, k.transpose(-2, -1))\n",
    "        att = att / math.sqrt(k.shape[-1])\n",
    "        mask = torch.tril(torch.ones(t, t, dtype=torch.bool)).view(1, 1, t, t)\n",
    "        #att = torch.where(mask, att, 0) masking attention to 0 is wrong \n",
    "        att = torch.where(mask, att, -1e9)\n",
    "        att = F.softmax(att, dim=-1) # (B, n_head, t, t)\n",
    "        y = torch.matmul(att, v) \n",
    "        #y = v.transpose(1, 2) typo found in original code\n",
    "        y = y.transpose(1, 2)\n",
    "        y = y.reshape(b, t, c)\n",
    "        y = self.out(y)\n",
    "        # <<< SECTION ENDS HERE.\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens, n_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hiddens)\n",
    "        self.ln2 = nn.LayerNorm(hiddens)\n",
    "        self.attn = CausalSelfAttention(hiddens, n_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hiddens, 4 * hiddens),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * hiddens, hiddens),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tokens=64,\n",
    "                 hiddens=64,\n",
    "                 n_heads=4,\n",
    "                 layers=4,\n",
    "                 vocab=256,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # >>> THIS SECTION CONTAINS A BUG.\n",
    "        self.layers = layers\n",
    "        self.tok_emb = nn.Embedding(vocab, hiddens)\n",
    "        # self.pos_emb = nn.Parameter(torch.empty(1, n_tokens, hiddens)) initialization of positional embedding is wrong\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, n_tokens, hiddens))\n",
    "        self.blocks = nn.ModuleList([Block(hiddens, n_heads) for _ in range(layers)])\n",
    "        self.ln_f = nn.LayerNorm(hiddens)\n",
    "        self.final = nn.Linear(hiddens, vocab, bias=False)\n",
    "        # <<< SECTION ENDS HERE.\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        token_embeddings = self.tok_emb(x)\n",
    "        position_embeddings = self.pos_emb[:, :t, :]\n",
    "        x = token_embeddings + position_embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.final(x)\n",
    "\n",
    "    def loss(self, tokens):\n",
    "        # >>> THIS SECTION CONTAINS A BUG.\n",
    "        # targets = tokens[:, :-1] targets should be the next token\n",
    "        targets = tokens[:, 1:]\n",
    "        logits = self.forward(tokens[:, :-1])\n",
    "        # print(targets.shape, logits.shape, logits.reshape(-1, logits.size(-1)))\n",
    "        return F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.flatten())\n",
    "        # <<< SECTION ENDS HERE.\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Trains the model and samples from it. There are no bugs here. #\n",
    "#################################################################\n",
    "\n",
    "def get_prompt():\n",
    "    text = torch.tensor(np.array([[ord(c) for c in '{:02d}_'.format(i)] for i in range(1, 16)]))\n",
    "    return text\n",
    "\n",
    "\n",
    "def sample(gpt):\n",
    "    print()\n",
    "    print('Samples:')\n",
    "    text = get_prompt()\n",
    "    for _ in range(60):\n",
    "        logits = gpt(text)[:, -1:]\n",
    "        new_tokens = torch.distributions.Categorical(F.softmax(logits / 0.8, dim=-1)).sample()\n",
    "        text = torch.cat([text, new_tokens], axis=1)\n",
    "    for t in text:\n",
    "        print(''.join([chr(c) for c in t]))\n",
    "\n",
    "\n",
    "def train():\n",
    "    data = [\n",
    "        \"09_Over every mistake\",\n",
    "        \"08_But there's no sense crying\",\n",
    "        \"07_Because we can\",\n",
    "        \"12_And the science gets done\",\n",
    "        \"04_My satisfaction\",\n",
    "        \"10_You just keep on trying\",\n",
    "        \"15_Still alive\",\n",
    "        \"01_This was a triumph\",\n",
    "        \"13_And you make a neat gun\",\n",
    "        \"11_Till you run out of cake\",\n",
    "        \"06_We do what we must\",\n",
    "        \"03_It's hard to overstate\",\n",
    "        \"14_For the people who are\",\n",
    "        \"05_Aperture Science:\",\n",
    "        \"02_I'm making a note here; 'Huge success'\",\n",
    "    ]\n",
    "    data = [[ord(c) for c in d] for d in data]\n",
    "    data = torch.tensor(np.array([np.pad(x, (0, 64 - len(x)), constant_values=ord('_')) for x in data]))\n",
    "\n",
    "    gpt = GPT()\n",
    "    optimizer = torch.optim.Adam(gpt.parameters(), lr=1e-2)\n",
    "    print('Loss:')\n",
    "    for i in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        l = gpt.loss(data)\n",
    "        if i % 10 == 0:\n",
    "            print('{:02d} {}'.format(i, l.detach().numpy()))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return gpt\n",
    "\n",
    "\n",
    "def run():\n",
    "    gpt = train()\n",
    "    sample(gpt)\n",
    "\n",
    "\n",
    "# model = CausalSelfAttention(64, 4)\n",
    "# x = torch.randn(5, 6, 64)\n",
    "# print(model(x).shape)\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08c320-bee5-4372-8612-ca4c865731e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f8111-0670-4106-9887-5990e3d9f749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
