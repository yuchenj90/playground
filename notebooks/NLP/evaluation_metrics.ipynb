{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ad486e-656e-4a43-9ef1-59fe14375fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import parse_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51663254-687a-4acf-819c-049f1f77e729",
   "metadata": {},
   "source": [
    "# Token-based metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaab20-46ee-46ad-97a3-44fd47c6d476",
   "metadata": {},
   "source": [
    "### WER (word error rate)\n",
    "- WER = (Substitutions + Insertions + Deletions) / Number of tokens. The closer WER to 0, the better.\n",
    "- Example: reference = ['I have a dog named Diudiu and he is smart.'], pred = ['I have a dog named Du Diu and he is very cute. ']. Pred needs 2 deletions, 2 substitutions to become reference, hence WER = (2+2)/10 = 0.4\n",
    "- WER is a widely adopted metric to evaluate ASR(automatic speech recognition) task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e07d16d8-6fc4-4c89-bae6-2b15ec6817b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "wer = load(\"wer\")\n",
    "\n",
    "ref = ['I have a dog named Diudiu and he is smart.']\n",
    "pred = ['I have a dog named Du Diu and he is very cute. ']\n",
    "wer_score = wer.compute(predictions=pred, references=ref)\n",
    "print(wer_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c86c28-0df9-42e8-aa3b-f5c72101c7a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bleu/Rouge score\n",
    "- Bleu measures the precision while rouge measures the recall. \n",
    "- $BLEUScore=BP∗exp(∑_{i=1}^k w_i∗\\ln(p_i))$, where $p_i$ is the precision for i-grams, $w_i$ is the normalized weight for i-gram. BP is a **brevity penalty** term defined by $BP = min\\{1, exp(1−\\frac{r}{c})\\}$, where r is the average length of reference and c is the  length of candidate.\n",
    "- $Rouge1 = \\frac{2}{(1/p_1 + 1/r_1)}$, where $p_1$ is the precision for 1-gram, and $r_1$ is the recall for 1-gram.\n",
    "- Example: reference = ['I have a dog named Diudiu and he is smart.'], pred = ['I have a dog named Du Diu and he is very cute. ']. Precision for 1-gram: 8/12 = 0.66, recall for 1-gram: 8/10=0.8. Thus, BLEU score for this example = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72967af8-981e-4263-a679-73325fed6550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "True\n",
      "0.00012340980408667956\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Define your desired weights (example: higher weight for bi-grams)\n",
    "weights = (1, 0, 0, 0)  # Weights for uni-gram, bi-gram, tri-gram, and 4-gram\n",
    " \n",
    "# Reference and predicted texts (same as before)\n",
    "ref = [[\"I\", \"have\", \"a\", \"dog\", \"named\", \"Diudiu\", \"and\", \"he\", \"is\", \"smart\"]]\n",
    "pred = [\"I\", \"have\", \"a\", \"dog\", \"named\", \"Du\", \"Diu\", \"and\", \"he\", \"is\", \"very\", \"cute\"]\n",
    " \n",
    "# Calculate BLEU score with weights\n",
    "score = sentence_bleu(ref, pred, weights=weights)\n",
    "print(score)\n",
    "print(score == np.minimum(1,np.exp(1-10/12))*np.exp(1*np.log(8/12)))\n",
    "pred = [\"and\"]\n",
    "score = sentence_bleu(ref, pred, weights=weights)\n",
    "print(score)\n",
    "print(score == np.exp(1-10/1)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04588774-9755-4fe6-8329-f0035654582e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.7272727272727272, 'rouge2': 0.6, 'rougeL': 0.7272727272727272, 'rougeLsum': 0.7272727272727272}\n",
      "-1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "ref = ['I have a dog named Diudiu and he is smart.']\n",
    "pred = ['I have a dog named Du Diu and he is very cute. ']\n",
    "results = rouge.compute(predictions=pred,\n",
    "                        references=ref)\n",
    "print(results)\n",
    "print(results['rouge1'] - 2/(1/(8/12)+1/(8/10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506c914-9b23-4e56-89b1-c19d9487013f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Meteor score\n",
    "- Consider two different words with the same stem the same.\n",
    "- $M = F_{mean}*(1-P)$, where $F_{mean} = PR/(\\alpha P + (1-\\alpha)R)$, i.e., recall weighed $\\frac{\\alpha}{1-\\alpha}$ times more than precision. \n",
    "- To take into account the extent to which the matched unigrams in the two strings are in the same word order, Meteor computes a penalty for a given alignment as follows. First, the sequence of matched unigrams between the two strings is divided into the fewest possible number of “chunks” such that the matched unigrams in each chunk are adjacent (in both strings) and in identical word order. The number of chunks (ch) and the number of matches (m) is then used to calculate a fragmentation fraction: frag = ch/m. The penalty is then computed as: $P = \\gamma frag^\\beta$. The value of $\\gamma$ determines the maximum penalty\n",
    "($0 ≤ \\gamma ≤ 1$). The value of $\\beta$ determines the functional relation between fragmentation and the penalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2382eeab-0703-46dd-8b94-89d9d3460c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778186274509804\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "ref = [\"I\", \"have\", \"a\", \"dog\", \"named\", \"Diudiu\", \"and\", \"he\", \"is\", \"smart\"]\n",
    "pred = [\"I\", \"have\", \"a\", \"dog\", \"named\", \"Du\", \"Diu\", \"and\", \"he\", \"is\", \"very\", \"cute\"]\n",
    "score = single_meteor_score(ref, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3925c26-c295-4b58-9e03-7b37334d5cf3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0msingle_meteor_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreference\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhypothesis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpreprocess\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m'lower'\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstemmer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStemmerI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwordnet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetCorpusReader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mWordNetCorpusReader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'/Users/yuchen.jiang2/nltk_data/corpora/wordnet.zip/wordnet/'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbeta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Calculates METEOR score for single hypothesis and reference as per\n",
       "\"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n",
       "Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n",
       "in Proceedings of ACL.\n",
       "https://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
       "\n",
       "\n",
       ">>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
       "\n",
       ">>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures', 'that', 'the', 'military', 'will', 'forever', 'heed', 'Party', 'commands']\n",
       "\n",
       "\n",
       ">>> round(single_meteor_score(reference1, hypothesis1),4)\n",
       "0.6944\n",
       "\n",
       "    If there is no words match during the alignment the method returns the\n",
       "    score as 0. We can safely  return a zero instead of raising a\n",
       "    division by zero error as no match usually implies a bad translation.\n",
       "\n",
       ">>> round(single_meteor_score(['this', 'is', 'a', 'cat'], ['non', 'matching', 'hypothesis']),4)\n",
       "0.0\n",
       "\n",
       ":param reference: pre-tokenized reference\n",
       ":param hypothesis: pre-tokenized hypothesis\n",
       ":param preprocess: preprocessing function (default str.lower)\n",
       ":param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
       ":param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
       ":param alpha: parameter for controlling relative weights of precision and recall.\n",
       ":param beta: parameter for controlling shape of penalty as a\n",
       "             function of as a function of fragmentation.\n",
       ":param gamma: relative weight assigned to fragmentation penalty.\n",
       ":return: The sentence-level METEOR score.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/personal/repos/playground/env/lib/python3.8/site-packages/nltk/translate/meteor_score.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_meteor_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252afd20-75dc-476a-b0b8-0e94cf05c90d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Semantic-based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef822c0f-bcf0-4cf2-a19e-19df1fc622c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BertScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe1f4e-fc23-40e4-b33b-97f306bb03b9",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"bertscore.png\" style=\"width: 1200px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c968c63a-2ae4-45ea-bed1-8ddfab79b8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "ref = ['I have a dog named Diudiu and he is smart.']\n",
    "pred = ['I have a dog named Du Diu and he is very cute. ']\n",
    "results = bertscore.compute(predictions=pred, references=ref, lang=\"en\", model_type=\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "57fd8b9a-8c78-40c4-a00b-768f4f35b4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, AutoModel, AutoTokenizer, BertConfig, GPT2Tokenizer, RobertaTokenizer, RobertaConfig\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "           output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a4702d15-dc7e-42fa-8e01-b5dad1b7c2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "lang2model = defaultdict(lambda: \"bert-base-multilingual-cased\")\n",
    "lang2model.update(\n",
    "    {\n",
    "        \"en\": \"roberta-large\",\n",
    "        \"zh\": \"bert-base-chinese\",\n",
    "        \"tr\": \"dbmdz/bert-base-turkish-cased\",\n",
    "        \"en-sci\": \"allenai/scibert_scivocab_uncased\",\n",
    "    }\n",
    ")\n",
    "model_type = lang2model['en']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_type)\n",
    "model.eval()\n",
    "print(len(model.encoder.layer))\n",
    "num_layers = 17\n",
    "model.encoder.layer = torch.nn.ModuleList([l for l in model.encoder.layer[:num_layers]])\n",
    "        \n",
    "idf_dict = defaultdict(lambda: 1.0)\n",
    "# set idf for [SEP] and [CLS] to 0\n",
    "idf_dict[tokenizer.sep_token_id] = 0\n",
    "idf_dict[tokenizer.cls_token_id] = 0\n",
    "idf_weights = [[idf_dict[i] for i in a] for a in arr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1dc50a3e-304c-46e6-abe4-8e9f7bc683de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padding(arr, pad_token, dtype=torch.long):\n",
    "    lens = torch.LongTensor([len(a) for a in arr])\n",
    "    max_len = lens.max().item()\n",
    "    padded = torch.ones(len(arr), max_len, dtype=dtype) * pad_token\n",
    "    mask = torch.zeros(len(arr), max_len, dtype=torch.long)\n",
    "    for i, a in enumerate(arr):\n",
    "        padded[i, : lens[i]] = torch.tensor(a, dtype=dtype)\n",
    "        mask[i, : lens[i]] = 1\n",
    "    return padded, lens, mask\n",
    "\n",
    "def collate_idf(arr, tokenizer, idf_dict, device=\"cuda:0\"):\n",
    "    \"\"\"\n",
    "    Helper function that pads a list of sentences to hvae the same length and\n",
    "    loads idf score for words in the sentences.\n",
    "\n",
    "    Args:\n",
    "        - :param: `arr` (list of str): sentences to process.\n",
    "        - :param: `tokenize` : a function that takes a string and return list\n",
    "                  of tokens.\n",
    "        - :param: `numericalize` : a function that takes a list of tokens and\n",
    "                  return list of token indexes.\n",
    "        - :param: `idf_dict` (dict): mapping a word piece index to its\n",
    "                               inverse document frequency\n",
    "        - :param: `pad` (str): the padding token.\n",
    "        - :param: `device` (str): device to use, e.g. 'cpu' or 'cuda'\n",
    "    \"\"\"\n",
    "    arr = [tokenizer.encode(\n",
    "                s.strip(),\n",
    "                add_special_tokens=True,\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            ) for s in arr]\n",
    "\n",
    "    idf_weights = [[idf_dict[i] for i in a] for a in arr]\n",
    "\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    padded, lens, mask = padding(arr, pad_token, dtype=torch.long)\n",
    "    padded_idf, _, _ = padding(idf_weights, 0, dtype=torch.float)\n",
    "\n",
    "    padded = padded.to(device=device)\n",
    "    mask = mask.to(device=device)\n",
    "    lens = lens.to(device=device)\n",
    "    return padded, padded_idf, lens, mask\n",
    "\n",
    "sent = ['I have a dog named Diudiu and he is smart.']\n",
    "padded_sens, padded_idf, lens, mask = collate_idf(\n",
    "        sent, tokenizer, idf_dict, device='cpu'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dbe6abff-4463-4ebf-b796-06a8f8d2f86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_encode(model, x, attention_mask, all_layers=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, attention_mask=attention_mask, output_hidden_states=all_layers)\n",
    "    if all_layers:\n",
    "        emb = torch.stack(out[-1], dim=2)\n",
    "    else:\n",
    "        emb = out[0]\n",
    "    return emb\n",
    "\n",
    "def get_bert_embedding(\n",
    "    all_sens,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    idf_dict,\n",
    "    batch_size=-1,\n",
    "    device=\"cuda:0\",\n",
    "    all_layers=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute BERT embedding in batches.\n",
    "\n",
    "    Args:\n",
    "        - :param: `all_sens` (list of str) : sentences to encode.\n",
    "        - :param: `model` : a BERT model from `pytorch_pretrained_bert`.\n",
    "        - :param: `tokenizer` : a BERT tokenizer corresponds to `model`.\n",
    "        - :param: `idf_dict` (dict) : mapping a word piece index to its\n",
    "                               inverse document frequency\n",
    "        - :param: `device` (str): device to use, e.g. 'cpu' or 'cuda'\n",
    "    \"\"\"\n",
    "\n",
    "    padded_sens, padded_idf, lens, mask = collate_idf(\n",
    "        all_sens, tokenizer, idf_dict, device=device\n",
    "    )\n",
    "\n",
    "    if batch_size == -1:\n",
    "        batch_size = len(all_sens)\n",
    "\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_sens), batch_size):\n",
    "            batch_embedding = bert_encode(\n",
    "                model,\n",
    "                padded_sens[i : i + batch_size],\n",
    "                attention_mask=mask[i : i + batch_size],\n",
    "                all_layers=all_layers,\n",
    "            )\n",
    "            embeddings.append(batch_embedding)\n",
    "            del batch_embedding\n",
    "\n",
    "    total_embedding = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    return total_embedding, mask, padded_idf\n",
    "\n",
    "ref = ['the weather is cold today', 'I have a dog named Diudiu and he is smart.']\n",
    "pred = ['it is freezing today', 'I have a dog named Du Diu and he is very cute. ']\n",
    "\n",
    "\n",
    "sents = ref+pred\n",
    "embs, masks, padded_idf = get_bert_embedding(\n",
    "        sents, model, tokenizer, idf_dict, device='cpu', all_layers=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9ae0375c-e5b1-4cac-9213-12c364b15cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 1024])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "72b57349-00cf-4ee2-b6f1-890089a0ee52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.1074e-03, -2.1795e-02,  2.9083e-01,  ..., -1.4929e-02,\n",
       "           2.1030e-02, -1.7272e-02],\n",
       "         [ 6.6264e-01, -2.1722e-01, -1.7440e+00,  ...,  3.8111e-01,\n",
       "           2.4545e-02, -6.9611e-02],\n",
       "         [-2.9132e-01, -3.9128e-02, -2.1329e+00,  ...,  1.6439e-01,\n",
       "          -8.2701e-01, -2.5847e-01],\n",
       "         ...,\n",
       "         [ 5.9117e-01, -4.4893e-01, -1.2562e+00,  ..., -6.5876e-02,\n",
       "          -7.7910e-01,  3.7352e-01],\n",
       "         [ 5.9117e-01, -4.4893e-01, -1.2562e+00,  ..., -6.5876e-02,\n",
       "          -7.7910e-01,  3.7352e-01],\n",
       "         [ 5.9117e-01, -4.4893e-01, -1.2562e+00,  ..., -6.5876e-02,\n",
       "          -7.7910e-01,  3.7352e-01]],\n",
       "\n",
       "        [[ 4.9547e-03, -2.3506e-02,  2.6005e-01,  ...,  1.6480e-01,\n",
       "          -1.5874e-01,  4.1766e-01],\n",
       "         [ 9.6563e-02,  1.2917e-01, -1.4965e+00,  ...,  3.7014e-01,\n",
       "          -3.2071e-02, -5.4490e-02],\n",
       "         [-8.7398e-02, -6.0991e-01, -2.7491e+00,  ...,  7.3609e-01,\n",
       "           1.1691e-01, -2.4989e-01],\n",
       "         ...,\n",
       "         [ 1.0244e-03, -1.9808e-03,  2.9674e-01,  ..., -1.3645e-02,\n",
       "           1.3597e-02, -2.5804e-02],\n",
       "         [-2.8640e-01, -2.6430e-01, -5.9603e-01,  ...,  1.3584e-01,\n",
       "          -2.1767e-01,  6.4769e-01],\n",
       "         [ 7.1236e-01, -5.7709e-01, -2.3846e+00,  ...,  6.6867e-02,\n",
       "          -3.4205e-01,  3.8684e-01]],\n",
       "\n",
       "        [[ 1.5927e-03, -2.0241e-02,  2.8286e-01,  ..., -2.2517e-02,\n",
       "           2.3478e-02, -1.8933e-02],\n",
       "         [ 5.5560e-01, -4.8222e-01, -1.3728e+00,  ...,  3.1910e-01,\n",
       "          -2.1553e-01,  3.0163e-01],\n",
       "         [-1.2315e-01, -5.4933e-01, -1.7711e+00,  ...,  3.0784e-03,\n",
       "          -5.1754e-01, -1.3494e-02],\n",
       "         ...,\n",
       "         [ 5.8502e-01, -3.3304e-01, -1.5661e+00,  ...,  1.9329e-01,\n",
       "          -5.6941e-01,  5.4752e-01],\n",
       "         [ 5.8502e-01, -3.3304e-01, -1.5661e+00,  ...,  1.9329e-01,\n",
       "          -5.6941e-01,  5.4752e-01],\n",
       "         [ 5.8502e-01, -3.3304e-01, -1.5661e+00,  ...,  1.9329e-01,\n",
       "          -5.6941e-01,  5.4752e-01]],\n",
       "\n",
       "        [[-2.5366e-03, -5.2278e-02,  1.9841e-01,  ...,  1.1596e-01,\n",
       "          -1.0829e-01,  4.2145e-01],\n",
       "         [ 6.9435e-02,  1.4785e-01, -1.1521e+00,  ...,  3.2255e-01,\n",
       "           1.2794e-02, -9.0212e-02],\n",
       "         [ 1.7270e-03, -6.4075e-01, -2.6052e+00,  ...,  8.0003e-01,\n",
       "          -3.1365e-02, -1.9319e-01],\n",
       "         ...,\n",
       "         [-4.9169e-01, -9.6690e-01, -2.0289e+00,  ..., -4.9731e-01,\n",
       "           8.2391e-02,  2.6426e-01],\n",
       "         [-1.9448e-03,  1.2364e-03,  2.9089e-01,  ..., -7.9992e-03,\n",
       "           1.6162e-02, -1.7399e-02],\n",
       "         [-3.2211e-01, -2.5142e-01, -5.9003e-01,  ...,  1.1399e-01,\n",
       "          -2.5088e-01,  6.3592e-01]]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "185f0278-1385-4ded-9abd-0d41a3e3260a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocesses text input in a way that BERT can interpret.\n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    # convert inputs to tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    return tokenized_text, tokens_tensor, segments_tensor\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "    # \"token\" is a [12 x 768] tensor\n",
    "    # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    return token_vecs_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5f7a0a8f-2777-4b57-b085-909cd77eb1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\"bank\",\n",
    "         \"he eventually sold the shares back to the bank at a premium.\",\n",
    "         \"the bank strongly resisted cutting interest rates.\",\n",
    "         \"the bank will supply and buy back foreign currency.\",\n",
    "         \"the bank is pressing us for repayment of the loan.\",\n",
    "         \"the bank left its lending rates unchanged.\",\n",
    "         \"the river flowed over the bank.\",\n",
    "         \"tall, luxuriant plants grew along the river bank.\",\n",
    "         \"his soldiers were arrayed along the river bank.\",\n",
    "         \"wild flowers adorned the river bank.\",\n",
    "         \"two fox cubs romped playfully on the river bank.\",\n",
    "         \"the jewels were kept in a bank vault.\",\n",
    "         \"you can stow your jewellery away in the bank.\",\n",
    "         \"most of the money was in storage in bank vaults.\",\n",
    "         \"the diamonds are shut away in a bank vault somewhere.\",\n",
    "         \"thieves broke into the bank vault.\",\n",
    "         \"can I bank on your support?\",\n",
    "         \"you can bank on him to hand you a reasonable bill for your services.\",\n",
    "         \"don't bank on your friends to help you out of trouble.\",\n",
    "         \"you can bank on me when you need money.\",\n",
    "         \"i bank on your help.\"\n",
    "         ]\n",
    "from collections import OrderedDict\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "for sentence in sentences:\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    # make ordered dictionary to keep track of the position of each   word\n",
    "    tokens = OrderedDict()\n",
    "    # loop over tokens in sensitive sentence\n",
    "    for token in tokenized_text[1:-1]:\n",
    "        # keep track of position of word and whether it occurs multiple times\n",
    "        if token in tokens:\n",
    "            tokens[token] += 1\n",
    "        else:\n",
    "            tokens[token] = 1\n",
    "    # compute the position of the current token\n",
    "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "    current_index = token_indices[tokens[token]-1]\n",
    "    # get the corresponding embedding\n",
    "    token_vec = list_token_embeddings[current_index]\n",
    "\n",
    "    # save values\n",
    "    context_tokens.append(token)\n",
    "    context_embeddings.append(token_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "88908b4d-57fa-47f0-8668-997966005efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emds = np.array([x.numpy() for x in context_embeddings])\n",
    "emds = emds/np.linalg.norm(emds, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6b1507f0-8289-4185-a9bb-c686b37da2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_similarity = (emds @ emds.transpose())\n",
    "plt.imshow(cos_similarity, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2956d193-4801-45dd-b24e-84525d5c6391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39182287"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity[7,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "baee127e-155f-4a30-8d05-acb4f1d8e96d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "76386683-38e9-465c-a6f1-760114d23483",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7359)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(context_embeddings[1], context_embeddings[2])/(context_embeddings[1].norm()*context_embeddings[2].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7575faa2-a566-47fb-8dbd-3b56019845ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7102)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(context_embeddings[1], context_embeddings[6])/(context_embeddings[1].norm()*context_embeddings[2].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774ec93-c591-4a92-831f-316a132a78b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e345a9c-2d10-433d-8166-e4430d8c1564",
   "metadata": {},
   "source": [
    "# Application example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40155e3b-b9c8-4e10-84a7-277c4b8e6601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/eval_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "667a6496-8059-4fc2-8ffe-6d1dd48b7b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['agent_first_msg'] = df['transcript'].map(lambda x: parse_transcript(x)[1]['agent_first_msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "464abdf0-6bd1-464a-818e-075a2f5345ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "refs = list(df['agent_first_msg'])\n",
    "preds = list(df['best_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "886a7584-af17-48c9-89c8-a140e8340857",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    }
   ],
   "source": [
    "results = bertscore.compute(predictions=preds, references=refs, lang=\"en\", model_type=\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ae97bb3-d365-4cfd-bacd-0edb88b010a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.8157134056091309,\n",
       "  0.7524710893630981,\n",
       "  0.7802231311798096,\n",
       "  0.8448470830917358,\n",
       "  0.8061633110046387,\n",
       "  0.683748722076416,\n",
       "  0.7758551239967346,\n",
       "  0.9291049242019653,\n",
       "  0.6675654053688049,\n",
       "  0.7445536255836487,\n",
       "  0.7399338483810425,\n",
       "  0.7711524963378906,\n",
       "  0.6948302984237671,\n",
       "  0.0,\n",
       "  0.7463991045951843,\n",
       "  0.715908408164978,\n",
       "  0.7032756209373474,\n",
       "  0.7548919916152954,\n",
       "  0.6953153610229492,\n",
       "  0.722528338432312,\n",
       "  0.8002091646194458,\n",
       "  0.7867988348007202,\n",
       "  0.760385274887085,\n",
       "  0.7243613004684448,\n",
       "  0.730371356010437,\n",
       "  0.6818893551826477,\n",
       "  0.6844093799591064,\n",
       "  0.7185503244400024,\n",
       "  0.7615588307380676,\n",
       "  0.6664412617683411,\n",
       "  0.0,\n",
       "  0.6789025068283081,\n",
       "  0.6500875949859619,\n",
       "  0.7217002511024475,\n",
       "  0.6647307276725769,\n",
       "  0.6542565822601318,\n",
       "  0.7077241539955139,\n",
       "  0.7533004283905029,\n",
       "  0.6962152719497681,\n",
       "  0.6942960023880005,\n",
       "  0.7417731881141663,\n",
       "  0.0,\n",
       "  0.6962890028953552,\n",
       "  0.722092866897583,\n",
       "  0.715508222579956,\n",
       "  0.64918452501297,\n",
       "  0.5945636034011841,\n",
       "  0.714802086353302,\n",
       "  0.601442277431488,\n",
       "  0.7646967768669128,\n",
       "  0.7618042826652527,\n",
       "  0.7001723051071167,\n",
       "  0.0,\n",
       "  0.7971640229225159,\n",
       "  0.7171868681907654,\n",
       "  0.7439965009689331,\n",
       "  0.7324209213256836,\n",
       "  0.0,\n",
       "  0.6153275966644287,\n",
       "  0.6243939399719238,\n",
       "  0.7335472106933594,\n",
       "  0.7519338130950928,\n",
       "  0.7406203150749207,\n",
       "  0.8259482979774475,\n",
       "  0.6412066221237183,\n",
       "  0.7700327038764954,\n",
       "  0.6836061477661133,\n",
       "  0.6688526272773743,\n",
       "  0.7486699819564819,\n",
       "  0.7674394249916077,\n",
       "  0.8006482720375061,\n",
       "  0.8000394105911255,\n",
       "  0.7609694600105286,\n",
       "  0.6932880878448486,\n",
       "  0.789926290512085,\n",
       "  0.8073790669441223,\n",
       "  0.7578837871551514,\n",
       "  0.750232994556427,\n",
       "  0.5990120768547058,\n",
       "  0.8027783632278442,\n",
       "  0.6807013750076294,\n",
       "  0.7493262887001038,\n",
       "  0.7450582385063171,\n",
       "  0.8035321831703186,\n",
       "  0.7294588088989258,\n",
       "  0.5775142908096313,\n",
       "  0.0,\n",
       "  0.6971778273582458,\n",
       "  0.6866580843925476,\n",
       "  0.6870740652084351,\n",
       "  0.6606625914573669,\n",
       "  0.7178350687026978,\n",
       "  0.7575942277908325,\n",
       "  0.7232836484909058,\n",
       "  0.6766633987426758,\n",
       "  0.703546941280365,\n",
       "  0.6979930400848389,\n",
       "  0.0,\n",
       "  0.6533313393592834,\n",
       "  0.84335857629776,\n",
       "  0.7705234885215759,\n",
       "  0.6837795376777649,\n",
       "  0.702660858631134,\n",
       "  0.84335857629776,\n",
       "  0.6653888821601868,\n",
       "  0.7169929146766663,\n",
       "  0.7436573505401611,\n",
       "  0.6554467678070068,\n",
       "  0.6417747735977173,\n",
       "  0.737220048904419,\n",
       "  0.6853315830230713,\n",
       "  0.6782139539718628,\n",
       "  0.7123914361000061,\n",
       "  0.7468710541725159,\n",
       "  0.7476165294647217,\n",
       "  0.692603349685669,\n",
       "  0.7107346057891846,\n",
       "  0.7791239023208618,\n",
       "  0.6980870366096497,\n",
       "  0.7600067853927612,\n",
       "  0.7490489482879639,\n",
       "  0.6877574920654297,\n",
       "  0.0,\n",
       "  0.6846850514411926,\n",
       "  0.7231006622314453,\n",
       "  0.7700377702713013,\n",
       "  0.7908826470375061,\n",
       "  0.7074950337409973,\n",
       "  0.7582789659500122,\n",
       "  0.7241423726081848,\n",
       "  0.7306942939758301,\n",
       "  0.7305281758308411,\n",
       "  0.7405065894126892,\n",
       "  0.0,\n",
       "  0.7708641886711121,\n",
       "  0.7018365859985352,\n",
       "  0.7275051474571228,\n",
       "  0.6829574108123779,\n",
       "  0.6696550846099854,\n",
       "  0.7508270144462585,\n",
       "  0.7089401483535767,\n",
       "  0.7052303552627563,\n",
       "  0.7630407810211182,\n",
       "  0.7287846803665161,\n",
       "  0.7557228803634644,\n",
       "  0.7003421783447266,\n",
       "  0.7727627158164978,\n",
       "  0.8016612529754639,\n",
       "  0.6403140425682068,\n",
       "  0.8383308053016663,\n",
       "  0.7945703268051147,\n",
       "  0.8618670701980591,\n",
       "  0.6525072455406189,\n",
       "  0.0,\n",
       "  0.6637688279151917,\n",
       "  0.7858811616897583,\n",
       "  0.6442044973373413,\n",
       "  0.7806282639503479,\n",
       "  0.8099193572998047,\n",
       "  0.7540509104728699,\n",
       "  0.6747740507125854,\n",
       "  0.6208194494247437,\n",
       "  0.7377339005470276,\n",
       "  0.6556732654571533,\n",
       "  0.6626444458961487,\n",
       "  0.6788506507873535,\n",
       "  0.0,\n",
       "  0.7087200880050659,\n",
       "  0.8272762298583984,\n",
       "  0.7219693064689636,\n",
       "  0.848816990852356,\n",
       "  0.7518839836120605,\n",
       "  0.8328490257263184,\n",
       "  0.7463984489440918,\n",
       "  0.6669539213180542,\n",
       "  0.6395120024681091,\n",
       "  0.7272569537162781,\n",
       "  0.0,\n",
       "  0.7843772172927856,\n",
       "  0.761120617389679,\n",
       "  0.7030624747276306,\n",
       "  0.7607057690620422,\n",
       "  0.7739841341972351,\n",
       "  0.7774248719215393,\n",
       "  0.7628436088562012,\n",
       "  0.6641784906387329,\n",
       "  0.7104825377464294,\n",
       "  0.7455636262893677,\n",
       "  0.7564274668693542,\n",
       "  0.7908408045768738,\n",
       "  0.6159281730651855,\n",
       "  0.5147731304168701,\n",
       "  0.7199891805648804,\n",
       "  0.7763457894325256,\n",
       "  0.7266151309013367,\n",
       "  0.7341081500053406,\n",
       "  0.788965106010437,\n",
       "  0.0,\n",
       "  0.7359227538108826,\n",
       "  0.712291419506073,\n",
       "  0.7373877763748169,\n",
       "  0.7964338064193726,\n",
       "  0.7532577514648438,\n",
       "  0.7487874031066895,\n",
       "  0.7604311108589172,\n",
       "  0.6489159464836121,\n",
       "  0.7256336212158203,\n",
       "  0.7174295783042908,\n",
       "  0.7276979684829712,\n",
       "  0.7306260466575623,\n",
       "  0.6968635320663452,\n",
       "  0.7468881011009216,\n",
       "  0.6025350093841553,\n",
       "  0.7287263870239258,\n",
       "  0.5682456493377686,\n",
       "  0.7250897884368896,\n",
       "  0.697603702545166,\n",
       "  0.6095105409622192,\n",
       "  0.6444997787475586,\n",
       "  0.6554142236709595,\n",
       "  0.0,\n",
       "  0.7476389408111572,\n",
       "  0.76664137840271,\n",
       "  0.6329034566879272,\n",
       "  0.6757505536079407,\n",
       "  0.6951333284378052,\n",
       "  0.696315586566925,\n",
       "  0.6443588137626648,\n",
       "  0.0,\n",
       "  0.8708159923553467,\n",
       "  0.6227362751960754,\n",
       "  0.6743413209915161,\n",
       "  0.6831282377243042,\n",
       "  0.6208603978157043,\n",
       "  0.7646507024765015,\n",
       "  0.7348119020462036,\n",
       "  0.8668959140777588,\n",
       "  0.8671919107437134,\n",
       "  0.7477021217346191,\n",
       "  0.7710633277893066,\n",
       "  0.7019982933998108,\n",
       "  0.7408071160316467,\n",
       "  0.7685313820838928,\n",
       "  0.7050402164459229,\n",
       "  0.7453002333641052,\n",
       "  0.7386395931243896,\n",
       "  0.7869783043861389,\n",
       "  0.5823456048965454,\n",
       "  0.7466400265693665,\n",
       "  0.7971019148826599,\n",
       "  0.6743459105491638,\n",
       "  0.7642179131507874,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.6654660105705261,\n",
       "  0.6319061517715454,\n",
       "  0.6981499195098877,\n",
       "  0.6719606518745422,\n",
       "  0.7640235424041748,\n",
       "  0.7254242300987244,\n",
       "  0.7292669415473938,\n",
       "  0.684162974357605,\n",
       "  0.6397683024406433,\n",
       "  0.735113263130188,\n",
       "  0.7961255311965942],\n",
       " 'recall': [0.7505662441253662,\n",
       "  0.6893468499183655,\n",
       "  0.7912242412567139,\n",
       "  0.7321302890777588,\n",
       "  0.7581658363342285,\n",
       "  0.7204780578613281,\n",
       "  0.7218500971794128,\n",
       "  0.8389517068862915,\n",
       "  0.6762420535087585,\n",
       "  0.7672567963600159,\n",
       "  0.6017274260520935,\n",
       "  0.7181521654129028,\n",
       "  0.568297803401947,\n",
       "  0.0,\n",
       "  0.6479934453964233,\n",
       "  0.6494083404541016,\n",
       "  0.7091600894927979,\n",
       "  0.7538162469863892,\n",
       "  0.6848511099815369,\n",
       "  0.5923131108283997,\n",
       "  0.7200747728347778,\n",
       "  0.8045638203620911,\n",
       "  0.7460264563560486,\n",
       "  0.6147373914718628,\n",
       "  0.7178306579589844,\n",
       "  0.6595772504806519,\n",
       "  0.6932468414306641,\n",
       "  0.6743239760398865,\n",
       "  0.729902982711792,\n",
       "  0.7036391496658325,\n",
       "  0.0,\n",
       "  0.6660172343254089,\n",
       "  0.6459617018699646,\n",
       "  0.6973537802696228,\n",
       "  0.6706450581550598,\n",
       "  0.6590535044670105,\n",
       "  0.6893395185470581,\n",
       "  0.704724133014679,\n",
       "  0.6879979372024536,\n",
       "  0.7052744626998901,\n",
       "  0.7585068941116333,\n",
       "  0.0,\n",
       "  0.7236911058425903,\n",
       "  0.7736719250679016,\n",
       "  0.6640939116477966,\n",
       "  0.6920156478881836,\n",
       "  0.636755108833313,\n",
       "  0.7312026023864746,\n",
       "  0.6200781464576721,\n",
       "  0.6816748380661011,\n",
       "  0.7208638191223145,\n",
       "  0.7025454044342041,\n",
       "  0.0,\n",
       "  0.7260740399360657,\n",
       "  0.7423475384712219,\n",
       "  0.7108509540557861,\n",
       "  0.7272105813026428,\n",
       "  0.0,\n",
       "  0.6288858652114868,\n",
       "  0.6316204071044922,\n",
       "  0.6856354475021362,\n",
       "  0.6935017704963684,\n",
       "  0.7435073256492615,\n",
       "  0.7502537965774536,\n",
       "  0.6136633157730103,\n",
       "  0.684671938419342,\n",
       "  0.6573226451873779,\n",
       "  0.653104841709137,\n",
       "  0.7096710801124573,\n",
       "  0.7322443723678589,\n",
       "  0.7190747261047363,\n",
       "  0.7080696225166321,\n",
       "  0.787964403629303,\n",
       "  0.6721580028533936,\n",
       "  0.758331835269928,\n",
       "  0.7166532874107361,\n",
       "  0.6657196283340454,\n",
       "  0.6918465495109558,\n",
       "  0.6213199496269226,\n",
       "  0.7990067601203918,\n",
       "  0.6637074947357178,\n",
       "  0.6603037714958191,\n",
       "  0.6716405749320984,\n",
       "  0.6803367733955383,\n",
       "  0.5985937118530273,\n",
       "  0.6778849363327026,\n",
       "  0.0,\n",
       "  0.7345555424690247,\n",
       "  0.7121925354003906,\n",
       "  0.7120693922042847,\n",
       "  0.6712724566459656,\n",
       "  0.7623375654220581,\n",
       "  0.7293479442596436,\n",
       "  0.7282087802886963,\n",
       "  0.7254253029823303,\n",
       "  0.6754018664360046,\n",
       "  0.7064635157585144,\n",
       "  0.0,\n",
       "  0.6317135095596313,\n",
       "  0.6953203082084656,\n",
       "  0.7885923385620117,\n",
       "  0.709308922290802,\n",
       "  0.6640514135360718,\n",
       "  0.6953203082084656,\n",
       "  0.6600328683853149,\n",
       "  0.7358757853507996,\n",
       "  0.7478545308113098,\n",
       "  0.6781028509140015,\n",
       "  0.6701984405517578,\n",
       "  0.7031225562095642,\n",
       "  0.6465629935264587,\n",
       "  0.6717573404312134,\n",
       "  0.7043652534484863,\n",
       "  0.7019590139389038,\n",
       "  0.7064457535743713,\n",
       "  0.6535441279411316,\n",
       "  0.7060310244560242,\n",
       "  0.7230226993560791,\n",
       "  0.6372107267379761,\n",
       "  0.7531510591506958,\n",
       "  0.7300916910171509,\n",
       "  0.6772028803825378,\n",
       "  0.0,\n",
       "  0.6661856174468994,\n",
       "  0.7269672155380249,\n",
       "  0.7429133057594299,\n",
       "  0.7419933080673218,\n",
       "  0.6899244785308838,\n",
       "  0.725951611995697,\n",
       "  0.6987914443016052,\n",
       "  0.6691223382949829,\n",
       "  0.7395437955856323,\n",
       "  0.5965743660926819,\n",
       "  0.0,\n",
       "  0.7373707294464111,\n",
       "  0.6947416067123413,\n",
       "  0.747215986251831,\n",
       "  0.6632879972457886,\n",
       "  0.6923213005065918,\n",
       "  0.7292864918708801,\n",
       "  0.7216142416000366,\n",
       "  0.6749495267868042,\n",
       "  0.7471193671226501,\n",
       "  0.7153365015983582,\n",
       "  0.7292991280555725,\n",
       "  0.7216978073120117,\n",
       "  0.7784488201141357,\n",
       "  0.7281725406646729,\n",
       "  0.6786626577377319,\n",
       "  0.7316324710845947,\n",
       "  0.6960522532463074,\n",
       "  0.7607768177986145,\n",
       "  0.6855435967445374,\n",
       "  0.0,\n",
       "  0.6507726907730103,\n",
       "  0.7299985885620117,\n",
       "  0.6464074850082397,\n",
       "  0.7445992231369019,\n",
       "  0.7002116441726685,\n",
       "  0.6527706980705261,\n",
       "  0.7260699272155762,\n",
       "  0.6761888265609741,\n",
       "  0.7369701266288757,\n",
       "  0.6877958178520203,\n",
       "  0.6675211191177368,\n",
       "  0.5461018681526184,\n",
       "  0.0,\n",
       "  0.7060580849647522,\n",
       "  0.7570745944976807,\n",
       "  0.7250686287879944,\n",
       "  0.7082410454750061,\n",
       "  0.6087906956672668,\n",
       "  0.7201500535011292,\n",
       "  0.6506822109222412,\n",
       "  0.6492606401443481,\n",
       "  0.6512028574943542,\n",
       "  0.7207287549972534,\n",
       "  0.0,\n",
       "  0.8254498243331909,\n",
       "  0.6964037418365479,\n",
       "  0.7637138962745667,\n",
       "  0.6167421936988831,\n",
       "  0.7146154642105103,\n",
       "  0.7281325459480286,\n",
       "  0.6767017841339111,\n",
       "  0.6488144993782043,\n",
       "  0.6873970627784729,\n",
       "  0.6768742203712463,\n",
       "  0.6900956034660339,\n",
       "  0.7483054399490356,\n",
       "  0.6907390356063843,\n",
       "  0.5999276638031006,\n",
       "  0.7198840975761414,\n",
       "  0.7205941677093506,\n",
       "  0.7148459553718567,\n",
       "  0.7067978978157043,\n",
       "  0.7318820357322693,\n",
       "  0.0,\n",
       "  0.7923665046691895,\n",
       "  0.691129207611084,\n",
       "  0.7073295712471008,\n",
       "  0.7885645627975464,\n",
       "  0.7808500528335571,\n",
       "  0.6855732798576355,\n",
       "  0.612263560295105,\n",
       "  0.6624301671981812,\n",
       "  0.7274766564369202,\n",
       "  0.7118983268737793,\n",
       "  0.7192021012306213,\n",
       "  0.6939808130264282,\n",
       "  0.664051353931427,\n",
       "  0.7514909505844116,\n",
       "  0.6290801167488098,\n",
       "  0.715349555015564,\n",
       "  0.6056115031242371,\n",
       "  0.7395095825195312,\n",
       "  0.6661088466644287,\n",
       "  0.6344213485717773,\n",
       "  0.6504113674163818,\n",
       "  0.6564798951148987,\n",
       "  0.0,\n",
       "  0.7968100309371948,\n",
       "  0.7002611756324768,\n",
       "  0.7181312441825867,\n",
       "  0.7254100441932678,\n",
       "  0.6749267578125,\n",
       "  0.7009167075157166,\n",
       "  0.6508919596672058,\n",
       "  0.0,\n",
       "  0.7453943490982056,\n",
       "  0.6343598961830139,\n",
       "  0.5792239904403687,\n",
       "  0.6657986044883728,\n",
       "  0.6629965901374817,\n",
       "  0.7178357243537903,\n",
       "  0.7087525725364685,\n",
       "  0.7895984649658203,\n",
       "  0.7646302580833435,\n",
       "  0.7392123937606812,\n",
       "  0.7411537766456604,\n",
       "  0.711748480796814,\n",
       "  0.7426939010620117,\n",
       "  0.7486557960510254,\n",
       "  0.6073254942893982,\n",
       "  0.7312191128730774,\n",
       "  0.6895549297332764,\n",
       "  0.727023184299469,\n",
       "  0.6114989519119263,\n",
       "  0.6701211333274841,\n",
       "  0.6923985481262207,\n",
       "  0.6694231033325195,\n",
       "  0.797277569770813,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.6878637671470642,\n",
       "  0.6395949125289917,\n",
       "  0.7017456293106079,\n",
       "  0.68357253074646,\n",
       "  0.7563721537590027,\n",
       "  0.733151376247406,\n",
       "  0.734336256980896,\n",
       "  0.6720207333564758,\n",
       "  0.6657021045684814,\n",
       "  0.7015883326530457,\n",
       "  0.6602128744125366],\n",
       " 'f1': [0.7817849516868591,\n",
       "  0.7195271253585815,\n",
       "  0.7856851816177368,\n",
       "  0.7844603657722473,\n",
       "  0.7814282178878784,\n",
       "  0.7016330361366272,\n",
       "  0.747878909111023,\n",
       "  0.8817298412322998,\n",
       "  0.6718757152557373,\n",
       "  0.7557347416877747,\n",
       "  0.6637122631072998,\n",
       "  0.7437092065811157,\n",
       "  0.6252264380455017,\n",
       "  0.0,\n",
       "  0.6937239766120911,\n",
       "  0.6810388565063477,\n",
       "  0.7062056064605713,\n",
       "  0.7543537020683289,\n",
       "  0.6900435090065002,\n",
       "  0.6509727835655212,\n",
       "  0.7580300569534302,\n",
       "  0.795582115650177,\n",
       "  0.7531374096870422,\n",
       "  0.6650621891021729,\n",
       "  0.7240467071533203,\n",
       "  0.6705477237701416,\n",
       "  0.6887997984886169,\n",
       "  0.6957350373268127,\n",
       "  0.7453949451446533,\n",
       "  0.6845352053642273,\n",
       "  0.0,\n",
       "  0.6723982095718384,\n",
       "  0.6480180621147156,\n",
       "  0.7093181610107422,\n",
       "  0.6676747798919678,\n",
       "  0.6566462516784668,\n",
       "  0.6984108686447144,\n",
       "  0.7282031178474426,\n",
       "  0.6920822262763977,\n",
       "  0.6997421383857727,\n",
       "  0.7500466704368591,\n",
       "  0.0,\n",
       "  0.7097256779670715,\n",
       "  0.7469930648803711,\n",
       "  0.6888430118560791,\n",
       "  0.6699162125587463,\n",
       "  0.6149365305900574,\n",
       "  0.7229093909263611,\n",
       "  0.6106180548667908,\n",
       "  0.7208031415939331,\n",
       "  0.7407687902450562,\n",
       "  0.701356828212738,\n",
       "  0.0,\n",
       "  0.7599601149559021,\n",
       "  0.7295503616333008,\n",
       "  0.7270461916923523,\n",
       "  0.7298064827919006,\n",
       "  0.0,\n",
       "  0.6220328211784363,\n",
       "  0.6279863715171814,\n",
       "  0.7087825536727905,\n",
       "  0.7215367555618286,\n",
       "  0.7420610189437866,\n",
       "  0.7862834930419922,\n",
       "  0.6271327137947083,\n",
       "  0.7248478531837463,\n",
       "  0.6702067852020264,\n",
       "  0.6608849167823792,\n",
       "  0.7286490201950073,\n",
       "  0.7494288682937622,\n",
       "  0.7576721906661987,\n",
       "  0.7512502074241638,\n",
       "  0.7742316722869873,\n",
       "  0.6825595498085022,\n",
       "  0.7738067507743835,\n",
       "  0.7593156695365906,\n",
       "  0.7088183760643005,\n",
       "  0.7198578119277954,\n",
       "  0.6099621653556824,\n",
       "  0.8008880615234375,\n",
       "  0.6720970273017883,\n",
       "  0.7020040154457092,\n",
       "  0.7064470052719116,\n",
       "  0.7368203997612,\n",
       "  0.6575785875320435,\n",
       "  0.623687207698822,\n",
       "  0.0,\n",
       "  0.7153787612915039,\n",
       "  0.699192225933075,\n",
       "  0.6993484497070312,\n",
       "  0.6659252643585205,\n",
       "  0.739417314529419,\n",
       "  0.743202805519104,\n",
       "  0.7257378697395325,\n",
       "  0.7001964449882507,\n",
       "  0.6891871690750122,\n",
       "  0.7022026777267456,\n",
       "  0.0,\n",
       "  0.6423405408859253,\n",
       "  0.7622179388999939,\n",
       "  0.7794532179832458,\n",
       "  0.69631028175354,\n",
       "  0.6828107833862305,\n",
       "  0.7622179388999939,\n",
       "  0.6626999974250793,\n",
       "  0.7263116836547852,\n",
       "  0.7457500696182251,\n",
       "  0.6665823459625244,\n",
       "  0.6556786894798279,\n",
       "  0.7197676301002502,\n",
       "  0.6653830409049988,\n",
       "  0.6749702095985413,\n",
       "  0.7083556652069092,\n",
       "  0.7237188816070557,\n",
       "  0.7264482975006104,\n",
       "  0.6725070476531982,\n",
       "  0.7083749175071716,\n",
       "  0.7500256896018982,\n",
       "  0.6662611365318298,\n",
       "  0.7565633654594421,\n",
       "  0.7394488453865051,\n",
       "  0.6824393272399902,\n",
       "  0.0,\n",
       "  0.6753087043762207,\n",
       "  0.7250287532806396,\n",
       "  0.7562323808670044,\n",
       "  0.7656582593917847,\n",
       "  0.6985993385314941,\n",
       "  0.7417632341384888,\n",
       "  0.7112410664558411,\n",
       "  0.6985541582107544,\n",
       "  0.7350082993507385,\n",
       "  0.6607935428619385,\n",
       "  0.0,\n",
       "  0.7537454962730408,\n",
       "  0.6982710957527161,\n",
       "  0.7372288107872009,\n",
       "  0.6729789972305298,\n",
       "  0.6807996034622192,\n",
       "  0.7399000525474548,\n",
       "  0.7152210474014282,\n",
       "  0.689757764339447,\n",
       "  0.754996120929718,\n",
       "  0.7219980359077454,\n",
       "  0.7422758936882019,\n",
       "  0.7108596563339233,\n",
       "  0.7755953669548035,\n",
       "  0.7631517648696899,\n",
       "  0.6589308977127075,\n",
       "  0.7813559174537659,\n",
       "  0.7420556545257568,\n",
       "  0.8081729412078857,\n",
       "  0.6686176061630249,\n",
       "  0.0,\n",
       "  0.6572065353393555,\n",
       "  0.7569098472595215,\n",
       "  0.6453041434288025,\n",
       "  0.762188196182251,\n",
       "  0.7510804533958435,\n",
       "  0.6997650861740112,\n",
       "  0.699482798576355,\n",
       "  0.6473222374916077,\n",
       "  0.7373517751693726,\n",
       "  0.6713504791259766,\n",
       "  0.665073812007904,\n",
       "  0.605283260345459,\n",
       "  0.0,\n",
       "  0.7073865532875061,\n",
       "  0.7906200885772705,\n",
       "  0.7235156893730164,\n",
       "  0.7721831798553467,\n",
       "  0.6728132963180542,\n",
       "  0.7724103331565857,\n",
       "  0.6952614784240723,\n",
       "  0.6579883694648743,\n",
       "  0.6453045010566711,\n",
       "  0.7239782214164734,\n",
       "  0.0,\n",
       "  0.8043895363807678,\n",
       "  0.727325439453125,\n",
       "  0.732134222984314,\n",
       "  0.6812008023262024,\n",
       "  0.7431159019470215,\n",
       "  0.7519717812538147,\n",
       "  0.7171953916549683,\n",
       "  0.656406581401825,\n",
       "  0.6987491846084595,\n",
       "  0.7095603942871094,\n",
       "  0.72174072265625,\n",
       "  0.7689854502677917,\n",
       "  0.6511920690536499,\n",
       "  0.5540978312492371,\n",
       "  0.7199366688728333,\n",
       "  0.7474318146705627,\n",
       "  0.7206825017929077,\n",
       "  0.7201942205429077,\n",
       "  0.7593523859977722,\n",
       "  0.0,\n",
       "  0.7631022930145264,\n",
       "  0.7015507221221924,\n",
       "  0.7220459580421448,\n",
       "  0.7924796342849731,\n",
       "  0.7668057680130005,\n",
       "  0.7157873511314392,\n",
       "  0.6783507466316223,\n",
       "  0.6556034088134766,\n",
       "  0.7265539765357971,\n",
       "  0.7146532535552979,\n",
       "  0.7234250903129578,\n",
       "  0.7118321061134338,\n",
       "  0.6800618767738342,\n",
       "  0.7491825222969055,\n",
       "  0.6155215501785278,\n",
       "  0.7219760417938232,\n",
       "  0.5863338112831116,\n",
       "  0.732228696346283,\n",
       "  0.6814925670623779,\n",
       "  0.6217164993286133,\n",
       "  0.647442102432251,\n",
       "  0.6559466123580933,\n",
       "  0.0,\n",
       "  0.7714417576789856,\n",
       "  0.7319493889808655,\n",
       "  0.6728291511535645,\n",
       "  0.6997002959251404,\n",
       "  0.6848810315132141,\n",
       "  0.6986085772514343,\n",
       "  0.6476088762283325,\n",
       "  0.0,\n",
       "  0.8032386898994446,\n",
       "  0.628494381904602,\n",
       "  0.6231740117073059,\n",
       "  0.6743521094322205,\n",
       "  0.6412370800971985,\n",
       "  0.7405039668083191,\n",
       "  0.7215470671653748,\n",
       "  0.8264437317848206,\n",
       "  0.8126880526542664,\n",
       "  0.7434329986572266,\n",
       "  0.7558128237724304,\n",
       "  0.7068397402763367,\n",
       "  0.7417493462562561,\n",
       "  0.758463442325592,\n",
       "  0.6525450348854065,\n",
       "  0.7381925582885742,\n",
       "  0.7132537961006165,\n",
       "  0.7558136582374573,\n",
       "  0.5965663194656372,\n",
       "  0.706314206123352,\n",
       "  0.7410701513290405,\n",
       "  0.6718754172325134,\n",
       "  0.7803977727890015,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.6764795780181885,\n",
       "  0.6357272863388062,\n",
       "  0.6999431848526001,\n",
       "  0.677716851234436,\n",
       "  0.7601785063743591,\n",
       "  0.7292672991752625,\n",
       "  0.7317928075790405,\n",
       "  0.6780374646186829,\n",
       "  0.6524775624275208,\n",
       "  0.7179597020149231,\n",
       "  0.7218272089958191],\n",
       " 'hashcode': 'distilbert-base-uncased_L5_no-idf_version=0.3.12(hug_trans=4.34.0)'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307f7da-3ebb-4814-9f0f-3f0b40f02cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
