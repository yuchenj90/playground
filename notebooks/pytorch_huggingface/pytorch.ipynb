{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11054809-d7d8-4c82-bc9d-e45afcd708a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizerFast, RobertaConfig, AutoTokenizer\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5978ff41-96d4-4e13-86a2-ad89f9f39f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/Airline_sentiment_raw.csv')\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f77cb-cf85-4abf-92f9-45c9d9110f64",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be39925f-1eb1-4f05-a128-773764c39129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array(list(df_train['text']))\n",
    "X_test = np.array(list(df_test['text']))\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "e = LabelEncoder()\n",
    "y_train = e.fit_transform(df_train['airline_sentiment'])\n",
    "y_test = e.transform(df_test['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600138ad-2d8c-41da-bd87-e9178d06a8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d88c9f0-4783-416d-b135-02eb01117638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        \n",
    "    def __len__(self):\n",
    "            return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "            label = self.labels[idx]\n",
    "            text = self.text[idx]\n",
    "            sample = {\"Text\": text, \"Class\": label}\n",
    "            return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2189419e-c432-4de6-b524-733b77c95420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomTextDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38715826-dff1-4df2-9f18-03fce1b79f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DL_DS = DataLoader(dataset, batch_size=2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb3a3d-a09e-471c-ba2c-5e8348651c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4afda2bd-9681-49cb-9b4d-d6d7fd38bd28",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76314115-dcbc-47b0-9905-572ee72bb34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class RobertaClassification(torch.nn.Module):\n",
    "    MODEL_ID = 'roberta-base'\n",
    "    DEFAULT_LOSS = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    #DEFAULT_OPT = torch.optim.Adam()\n",
    "    \n",
    "    def __init__(self, hidden_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained(self.MODEL_ID)\n",
    "        self.roberta = RobertaModel.from_pretrained(self.MODEL_ID)\n",
    "        self.layers = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.layers.append((f'conv{i}', torch.nn.Linear(hidden_dims[i],hidden_dims[i+1])))\n",
    "            self.layers.append((f'relu{i}', torch.nn.ReLU()))\n",
    "        self.layers.append(('final_layer', torch.nn.Linear(hidden_dims[-1], 3)))\n",
    "    \n",
    "    def build(self) -> None:\n",
    "        def init_normal(m):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                torch.nn.init.uniform_(m.weight)\n",
    "        self.model = torch.nn.Sequential(OrderedDict(self.layers))\n",
    "        self.model.apply(init_normal)\n",
    "\n",
    "    def forward(self, x: str):\n",
    "        x = self.tokenizer(x, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        x = self.roberta(**x).pooler_output\n",
    "        return self.model(x)\n",
    "    \n",
    "    def train(self, \n",
    "              batch_size: int = 64, \n",
    "              loss: torch.nn.modules.loss = DEFAULT_LOSS,\n",
    "              #optimizer: torch.nn.modules = self.DEFAULT_OPT,\n",
    "             ):\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d319c224-5cd7-48fc-8659-0f420828239f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5af564abf3547fcace0bd8938b2ddd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "rc = RobertaClassification([768,512,128,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e675f6dc-7788-4e7a-b9b5-75da0d71f7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[866535.1250, 892398.8750, 771191.6875]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.build()\n",
    "rc(['How are you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7f9ebc-fded-4388-95f2-d2665fc1b90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 500\n",
    "BATCH_SIZE = 256\n",
    "dl = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(rc.parameters())\n",
    "dataset_size = len(dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb7d2f-4918-411e-b275-ca0f29585a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 43585.707031  [  256/11712]\n",
      "train loss: 948.493097    train acc: 0.581748\n",
      "test loss: 0.617188    test acc: 0.617188\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.912096  [  256/11712]\n",
      "train loss: 0.914123    train acc: 0.627944\n",
      "test loss: 0.617188    test acc: 0.617188\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.882418  [  256/11712]\n",
      "train loss: 0.914145    train acc: 0.628199\n",
      "test loss: 0.617188    test acc: 0.617188\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.949663  [  256/11712]\n",
      "train loss: 0.914030    train acc: 0.628199\n",
      "test loss: 0.617188    test acc: 0.617188\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.916506  [  256/11712]\n",
      "train loss: 0.914171    train acc: 0.628029\n",
      "test loss: 0.617188    test acc: 0.617188\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.943697  [  256/11712]\n",
      "train loss: 0.914420    train acc: 0.627831\n",
      "test loss: 0.617188    test acc: 0.617188\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.946919  [  256/11712]\n",
      "train loss: 0.913932    train acc: 0.628085\n",
      "test loss: 0.617188    test acc: 0.617188\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.893726  [  256/11712]\n",
      "train loss: 0.914217    train acc: 0.628142\n",
      "test loss: 0.617188    test acc: 0.617188\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.921668  [  256/11712]\n"
     ]
    }
   ],
   "source": [
    "train_loss_hist = []\n",
    "train_acc_hist = []\n",
    "test_loss_hist = []\n",
    "test_acc_hist = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    # set model in training mode and run through each batch\n",
    "    rc.train()\n",
    "    \n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for id_batch, batched_data in enumerate(dl):\n",
    "        x_batch = batched_data['Text']\n",
    "        y_batch = batched_data['Class']\n",
    "\n",
    "        y_batch_pred = rc(x_batch)\n",
    "\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (torch.argmax(y_batch_pred, 1) == y_batch).float().mean()\n",
    "        epoch_loss.append(float(loss))\n",
    "        epoch_acc.append(float(acc))\n",
    "        # Every 100 batches, print the loss for this batch\n",
    "        # as well as the number of examples processed so far \n",
    "        if id_batch % 100 == 0:\n",
    "            loss, current = loss.item(), (id_batch + 1)* len(x_batch)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "    \n",
    "    rc.eval()\n",
    "    y_pred = rc(list(X_test[:256]))\n",
    "    ce = loss_fn(y_pred, torch.from_numpy(y_test[:256]))\n",
    "    acc = (torch.argmax(y_pred, 1) == torch.from_numpy(y_test[:256])).float().mean()\n",
    "    \n",
    "    train_loss_hist.append(np.mean(epoch_loss))\n",
    "    train_acc_hist.append(np.mean(epoch_acc))\n",
    "    test_loss_hist.append(ce)\n",
    "    test_acc_hist.append(acc)\n",
    "    \n",
    "    print(f\"train loss: {train_loss_hist[-1]:>7f}    train acc: {train_acc_hist[-1]:>5f}\")\n",
    "    print(f\"test loss: {test_acc_hist[-1]:>7f}    test acc: {test_acc_hist[-1]:>5f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddce3626-01d5-4ef7-99c3-9cd4d21039de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my_test\u001b[49m[:\u001b[38;5;241m256\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test[:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2dad28d-7349-42e6-b1fc-c7b09fbcc0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 1, 0, 1,\n",
      "        2, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 2, 2, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 2, 0,\n",
      "        2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 2, 0, 0, 0, 2, 0, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0,\n",
      "        1, 0, 0, 2, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 1,\n",
      "        0, 2, 2, 0, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "84fddcee-d7f0-4f56-9c90-567d39214c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461],\n",
      "        [12.3881, 11.3308, 11.0461]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y_batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b14a25-d287-403f-a75c-b656baa935d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n",
    "roberta_model = RobertaModel.from_pretrained(model_id)\n",
    "\n",
    "# This function tokenizes the input text using the RoBERTa tokenizer. \n",
    "# It applies padding and truncation to ensure that all sequences have the same length (256 tokens).\n",
    "def get_embedding(x):\n",
    "    ins = tokenizer(x, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return roberta_model(**ins).pooler_output\n",
    "\n",
    "# X_train = get_embedding(list(df_train['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34e21b3-ff4b-43c1-b2a3-123333c6b802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "n_batches = len(df_train)//batch_size + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22b1f163-2fac-46a0-b37c-2b157cf5a5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        i0 = batch_size*i\n",
    "        i1 = min([batch_size*(i+1),len(df_train)])\n",
    "        yield get_embedding(list(df_train.iloc[i0:i1]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a357c1c-eb0d-4980-8934-0e4d5be80912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████▏                                                                          | 3/23 [00:42<04:58, 14.90s/it]"
     ]
    }
   ],
   "source": [
    "outs = [torch.tensor([0]) for i in range(n_batches)]\n",
    "i = 0\n",
    "for data in gen_data():\n",
    "    outs[i] = data\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a70479-7d03-4e3c-bfc8-d374209bbc95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab77996-227f-4736-a78d-2627fccfedbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'params'"
     ]
    }
   ],
   "source": [
    "type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c471f927-1492-4c99-80f4-a7ff909722d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "rc = RobertaClassification([768,512,128,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c275ef5f-55b5-4e69-832d-d2289cb298ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18.7333]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.build()\n",
    "rc.forward(['How are you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abb384ff-458b-41aa-8ce0-d2a8d33c5247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(rc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3b7cb-cff1-4a4b-939a-87872844cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "    y_pred = rc.forward(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eec1701-1772-4124-934d-603f2878b554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x2f3503dd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f2b91c6-ff4c-4c49-9cf5-269152de4e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae943c1-0985-4b89-8b48-20ac767a1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54eae91f-7937-4d32-b7d9-65b51a00cf95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RobertaClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rc \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaClassification\u001b[49m([\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RobertaClassification' is not defined"
     ]
    }
   ],
   "source": [
    "rc = RobertaClassification([768, 512, 128, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1d9879-2e03-4634-b83b-8b6e6ed931c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rc.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "568aa8cb-2f9a-4ce0-a010-62e4bd5377a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = list(df_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f53c7-6b16-4526-8ba6-093d65f8601d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rc.forward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b86dc-6ec9-435e-886e-2206cc506b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb76c49-524b-4d73-a4f0-2f37d5bf4774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam()\n",
    "for t in range(100):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = rc(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "516231f5-1279-47b7-8a8e-ee99c1a8a511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy dog is cute!\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 13\u001b[0m, in \u001b[0;36mRobertaClassification.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:791\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 791\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    792\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/transformers/modeling_utils.py:3821\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   3818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 3821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m   3822\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3823\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3824\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3826\u001b[0m     )\n\u001b[1;32m   3828\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   3829\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "rc.forward('my dog is cute!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1010dba-3eb1-4ff5-8848-022fff6f9fed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "hidden_dims = [768,512,128,64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa92126a-abb2-4caf-954b-9f2aeda93ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pretrained',\n",
       "              RobertaModel(\n",
       "                (embeddings): RobertaEmbeddings(\n",
       "                  (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "                  (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "                  (token_type_embeddings): Embedding(1, 768)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (encoder): RobertaEncoder(\n",
       "                  (layer): ModuleList(\n",
       "                    (0-11): 12 x RobertaLayer(\n",
       "                      (attention): RobertaAttention(\n",
       "                        (self): RobertaSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): RobertaSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): RobertaIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): RobertaOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (pooler): RobertaPooler(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (activation): Tanh()\n",
       "                )\n",
       "              )),\n",
       "             ('conv0', Linear(in_features=768, out_features=512, bias=True)),\n",
       "             ('relu0', ReLU()),\n",
       "             ('conv1', Linear(in_features=512, out_features=128, bias=True)),\n",
       "             ('relu1', ReLU()),\n",
       "             ('conv2', Linear(in_features=128, out_features=64, bias=True)),\n",
       "             ('sigmoid', Sigmoid())])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ceb36c06-e64d-4b34-90b2-3db69203ee57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pretrained',\n",
       "  RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )),\n",
       " ('conv0', Linear(in_features=768, out_features=512, bias=True)),\n",
       " ('relu0', ReLU()),\n",
       " ('conv1', Linear(in_features=512, out_features=128, bias=True)),\n",
       " ('relu1', ReLU()),\n",
       " ('conv2', Linear(in_features=128, out_features=64, bias=True)),\n",
       " ('relu2', ReLU())]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7500c7b3-e43f-49f7-bc74-3c119d9f5a72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A sequential container.\n",
       "Modules will be added to it in the order they are passed in the\n",
       "constructor. Alternatively, an ``OrderedDict`` of modules can be\n",
       "passed in. The ``forward()`` method of ``Sequential`` accepts any\n",
       "input and forwards it to the first module it contains. It then\n",
       "\"chains\" outputs to inputs sequentially for each subsequent module,\n",
       "finally returning the output of the last module.\n",
       "\n",
       "The value a ``Sequential`` provides over manually calling a sequence\n",
       "of modules is that it allows treating the whole container as a\n",
       "single module, such that performing a transformation on the\n",
       "``Sequential`` applies to each of the modules it stores (which are\n",
       "each a registered submodule of the ``Sequential``).\n",
       "\n",
       "What's the difference between a ``Sequential`` and a\n",
       ":class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it\n",
       "sounds like--a list for storing ``Module`` s! On the other hand,\n",
       "the layers in a ``Sequential`` are connected in a cascading way.\n",
       "\n",
       "Example::\n",
       "\n",
       "    # Using Sequential to create a small model. When `model` is run,\n",
       "    # input will first be passed to `Conv2d(1,20,5)`. The output of\n",
       "    # `Conv2d(1,20,5)` will be used as the input to the first\n",
       "    # `ReLU`; the output of the first `ReLU` will become the input\n",
       "    # for `Conv2d(20,64,5)`. Finally, the output of\n",
       "    # `Conv2d(20,64,5)` will be used as input to the second `ReLU`\n",
       "    model = nn.Sequential(\n",
       "              nn.Conv2d(1,20,5),\n",
       "              nn.ReLU(),\n",
       "              nn.Conv2d(20,64,5),\n",
       "              nn.ReLU()\n",
       "            )\n",
       "\n",
       "    # Using Sequential with OrderedDict. This is functionally the\n",
       "    # same as the above code\n",
       "    model = nn.Sequential(OrderedDict([\n",
       "              ('conv1', nn.Conv2d(1,20,5)),\n",
       "              ('relu1', nn.ReLU()),\n",
       "              ('conv2', nn.Conv2d(20,64,5)),\n",
       "              ('relu2', nn.ReLU())\n",
       "            ]))\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/container.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     _FusedModule"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7b4e7b8-bae4-4b2e-b98c-7f27f6c03d38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv0): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (relu0): ReLU()\n",
       "  (conv1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Sequential(hl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd9eb7d0-2de9-4a19-925b-5ed1d4252de1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     hidden_layers\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(hidden_dims[i],hidden_dims[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m      5\u001b[0m     hidden_layers\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU())\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/container.py:104\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args):\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/module.py:596\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Adds a child module to the current module.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, Module) \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not a Module subclass\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    597\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtypename(module)))\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule name should be a string. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    600\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtypename(name)))\n",
      "\u001b[0;31mTypeError\u001b[0m: list is not a Module subclass"
     ]
    }
   ],
   "source": [
    "hidden_layers = []\n",
    "hidden_dims = [768,512,128,64]\n",
    "for i in range(len(hidden_dims)-1):\n",
    "    hidden_layers.append(torch.nn.Linear(hidden_dims[i],hidden_dims[i+1]))\n",
    "    hidden_layers.append(torch.nn.ReLU())\n",
    "torch.nn.Sequential(hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91f761a9-21ba-4826-b4e0-49353e85973c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mRobertaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforce_download\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_safetensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
       "\n",
       "The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n",
       "the model, you should first set it back in training mode with `model.train()`.\n",
       "\n",
       "The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
       "pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
       "task.\n",
       "\n",
       "The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
       "weights are discarded.\n",
       "\n",
       "Parameters:\n",
       "    pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
       "        Can be either:\n",
       "\n",
       "            - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
       "              Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
       "              user or organization name, like `dbmdz/bert-base-german-cased`.\n",
       "            - A path to a *directory* containing model weights saved using\n",
       "              [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
       "            - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
       "              this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
       "              `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
       "              PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
       "            - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n",
       "              `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n",
       "              `True`.\n",
       "            - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
       "              arguments `config` and `state_dict`).\n",
       "    model_args (sequence of positional arguments, *optional*):\n",
       "        All remaining positional arguments will be passed to the underlying model's `__init__` method.\n",
       "    config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n",
       "        Can be either:\n",
       "\n",
       "            - an instance of a class derived from [`PretrainedConfig`],\n",
       "            - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n",
       "\n",
       "        Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
       "        be automatically loaded when:\n",
       "\n",
       "            - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
       "              model).\n",
       "            - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
       "              save directory.\n",
       "            - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
       "              configuration JSON file named *config.json* is found in the directory.\n",
       "    state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
       "        A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
       "\n",
       "        This option can be used if you want to create a model from a pretrained configuration but load your own\n",
       "        weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
       "        [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
       "    cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
       "        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
       "        standard cache should not be used.\n",
       "    from_tf (`bool`, *optional*, defaults to `False`):\n",
       "        Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
       "        `pretrained_model_name_or_path` argument).\n",
       "    from_flax (`bool`, *optional*, defaults to `False`):\n",
       "        Load the model weights from a Flax checkpoint save file (see docstring of\n",
       "        `pretrained_model_name_or_path` argument).\n",
       "    ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n",
       "        as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n",
       "        checkpoint with 3 labels).\n",
       "    force_download (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
       "        cached versions if they exist.\n",
       "    resume_download (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
       "        file exists.\n",
       "    proxies (`Dict[str, str]`, *optional*):\n",
       "        A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
       "        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
       "    output_loading_info(`bool`, *optional*, defaults to `False`):\n",
       "        Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
       "    local_files_only(`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to only look at local files (i.e., do not try to download the model).\n",
       "    token (`str` or `bool`, *optional*):\n",
       "        The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n",
       "        the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
       "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
       "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
       "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
       "        identifier allowed by git.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    mirror (`str`, *optional*):\n",
       "        Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
       "        problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
       "        Please refer to the mirror site for more information.\n",
       "    _fast_init(`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to disable fast initialization.\n",
       "\n",
       "        <Tip warning={true}>\n",
       "\n",
       "        One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\n",
       "        4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\n",
       "        [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    > Parameters for big model inference\n",
       "\n",
       "    low_cpu_mem_usage(`bool`, *optional*):\n",
       "        Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
       "        This is an experimental feature and a subject to change at any moment.\n",
       "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
       "        Override the default `torch.dtype` and load the model under a specific `dtype`. The different options\n",
       "        are:\n",
       "\n",
       "        1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\n",
       "          `dtype`, ignoring the model's `config.torch_dtype` if one exists. If not specified\n",
       "          - the model will get loaded in `torch.float` (fp32).\n",
       "\n",
       "        2. `\"auto\"` - A `torch_dtype` entry in the `config.json` file of the model will be\n",
       "          attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\n",
       "          the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\n",
       "          using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\n",
       "          the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\n",
       "        reach out to the authors and ask them to add this information to the model's card and to insert the\n",
       "        `torch_dtype` entry in `config.json` on the hub.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n",
       "        A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
       "        parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
       "        same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n",
       "        like `1`) on which the model will be allocated, the device map will map the entire model to this\n",
       "        device. Passing `device_map = 0` means put the whole model on GPU 0.\n",
       "\n",
       "        To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
       "        more information about each option see [designing a device\n",
       "        map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
       "    max_memory (`Dict`, *optional*):\n",
       "        A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
       "        GPU and the available CPU RAM if unset.\n",
       "    offload_folder (`str` or `os.PathLike`, *optional*):\n",
       "        If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
       "    offload_state_dict (`bool`, *optional*):\n",
       "        If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\n",
       "        RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\n",
       "        `True` when there is some disk offload.\n",
       "    load_in_8bit (`bool`, *optional*, defaults to `False`):\n",
       "        If `True`, will convert the loaded model into mixed-8bit quantized model. To use this feature please\n",
       "        install `bitsandbytes` (`pip install -U bitsandbytes`).\n",
       "    load_in_4bit (`bool`, *optional*, defaults to `False`):\n",
       "        If `True`, will convert the loaded model into 4bit precision quantized model. To use this feature\n",
       "        install the latest version of `bitsandbytes` (`pip install -U bitsandbytes`).\n",
       "    quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\n",
       "        A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\n",
       "        bitsandbytes, gptq)\n",
       "    subfolder (`str`, *optional*, defaults to `\"\"`):\n",
       "        In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
       "        specify the folder name here.\n",
       "    variant (`str`, *optional*):\n",
       "        If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is\n",
       "        ignored when using `from_tf` or `from_flax`.\n",
       "    use_safetensors (`bool`, *optional*, defaults to `None`):\n",
       "        Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\n",
       "        is not installed, it will be set to `False`.\n",
       "\n",
       "    kwargs (remaining dictionary of keyword arguments, *optional*):\n",
       "        Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
       "        `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
       "        automatically loaded:\n",
       "\n",
       "            - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
       "              underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
       "              already been done)\n",
       "            - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
       "              initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
       "              corresponds to a configuration attribute will be used to override said attribute with the\n",
       "              supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
       "              will be passed to the underlying model's `__init__` function.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n",
       "use this method in a firewalled environment.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Examples:\n",
       "\n",
       "```python\n",
       ">>> from transformers import BertConfig, BertModel\n",
       "\n",
       ">>> # Download model and configuration from huggingface.co and cache.\n",
       ">>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
       ">>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n",
       ">>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n",
       ">>> # Update configuration during loading.\n",
       ">>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
       ">>> assert model.config.output_attentions == True\n",
       ">>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
       ">>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n",
       ">>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n",
       ">>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n",
       ">>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\n",
       "```\n",
       "\n",
       "* `low_cpu_mem_usage` algorithm:\n",
       "\n",
       "This is an experimental function that loads the model using ~1x model size CPU memory\n",
       "\n",
       "Here is how it works:\n",
       "\n",
       "1. save which state_dict keys we have\n",
       "2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\n",
       "3. after the model has been instantiated switch to the meta device all params/buffers that\n",
       "are going to be replaced from the loaded state_dict\n",
       "4. load state_dict 2nd time\n",
       "5. replace the params/buffers from the state_dict\n",
       "\n",
       "Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\n",
       "\u001b[0;31mFile:\u001b[0m      ~/personal/repos/playground/env/lib/python3.8/site-packages/transformers/modeling_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RobertaModel.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b519ac8f-568e-4cf5-8dbe-44f11f9c377d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies the rectified linear unit function element-wise:\n",
       "\n",
       ":math:`\\text{ReLU}(x) = (x)^+ = \\max(0, x)`\n",
       "\n",
       "Args:\n",
       "    inplace: can optionally do the operation in-place. Default: ``False``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
       "    - Output: :math:`(*)`, same shape as the input.\n",
       "\n",
       ".. image:: ../scripts/activation_images/ReLU.png\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.ReLU()\n",
       "    >>> input = torch.randn(2)\n",
       "    >>> output = m(input)\n",
       "\n",
       "\n",
       "  An implementation of CReLU - https://arxiv.org/abs/1603.05201\n",
       "\n",
       "    >>> m = nn.ReLU()\n",
       "    >>> input = torch.randn(2).unsqueeze(0)\n",
       "    >>> output = torch.cat((m(input), m(-input)))\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/activation.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     ReLU6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.ReLU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4376736-e930-424f-bf3c-914fdcdf874a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mRobertaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforce_download\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_safetensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
       "\n",
       "The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n",
       "the model, you should first set it back in training mode with `model.train()`.\n",
       "\n",
       "The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
       "pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
       "task.\n",
       "\n",
       "The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
       "weights are discarded.\n",
       "\n",
       "Parameters:\n",
       "    pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
       "        Can be either:\n",
       "\n",
       "            - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
       "              Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
       "              user or organization name, like `dbmdz/bert-base-german-cased`.\n",
       "            - A path to a *directory* containing model weights saved using\n",
       "              [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
       "            - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
       "              this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
       "              `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
       "              PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
       "            - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n",
       "              `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n",
       "              `True`.\n",
       "            - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
       "              arguments `config` and `state_dict`).\n",
       "    model_args (sequence of positional arguments, *optional*):\n",
       "        All remaining positional arguments will be passed to the underlying model's `__init__` method.\n",
       "    config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n",
       "        Can be either:\n",
       "\n",
       "            - an instance of a class derived from [`PretrainedConfig`],\n",
       "            - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n",
       "\n",
       "        Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
       "        be automatically loaded when:\n",
       "\n",
       "            - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
       "              model).\n",
       "            - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
       "              save directory.\n",
       "            - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
       "              configuration JSON file named *config.json* is found in the directory.\n",
       "    state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
       "        A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
       "\n",
       "        This option can be used if you want to create a model from a pretrained configuration but load your own\n",
       "        weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
       "        [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
       "    cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
       "        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
       "        standard cache should not be used.\n",
       "    from_tf (`bool`, *optional*, defaults to `False`):\n",
       "        Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
       "        `pretrained_model_name_or_path` argument).\n",
       "    from_flax (`bool`, *optional*, defaults to `False`):\n",
       "        Load the model weights from a Flax checkpoint save file (see docstring of\n",
       "        `pretrained_model_name_or_path` argument).\n",
       "    ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n",
       "        as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n",
       "        checkpoint with 3 labels).\n",
       "    force_download (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
       "        cached versions if they exist.\n",
       "    resume_download (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
       "        file exists.\n",
       "    proxies (`Dict[str, str]`, *optional*):\n",
       "        A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
       "        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
       "    output_loading_info(`bool`, *optional*, defaults to `False`):\n",
       "        Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
       "    local_files_only(`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to only look at local files (i.e., do not try to download the model).\n",
       "    token (`str` or `bool`, *optional*):\n",
       "        The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n",
       "        the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
       "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
       "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
       "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
       "        identifier allowed by git.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    mirror (`str`, *optional*):\n",
       "        Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
       "        problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
       "        Please refer to the mirror site for more information.\n",
       "    _fast_init(`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to disable fast initialization.\n",
       "\n",
       "        <Tip warning={true}>\n",
       "\n",
       "        One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\n",
       "        4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\n",
       "        [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    > Parameters for big model inference\n",
       "\n",
       "    low_cpu_mem_usage(`bool`, *optional*):\n",
       "        Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
       "        This is an experimental feature and a subject to change at any moment.\n",
       "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
       "        Override the default `torch.dtype` and load the model under a specific `dtype`. The different options\n",
       "        are:\n",
       "\n",
       "        1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\n",
       "          `dtype`, ignoring the model's `config.torch_dtype` if one exists. If not specified\n",
       "          - the model will get loaded in `torch.float` (fp32).\n",
       "\n",
       "        2. `\"auto\"` - A `torch_dtype` entry in the `config.json` file of the model will be\n",
       "          attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\n",
       "          the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\n",
       "          using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\n",
       "          the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\n",
       "        reach out to the authors and ask them to add this information to the model's card and to insert the\n",
       "        `torch_dtype` entry in `config.json` on the hub.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n",
       "        A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
       "        parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
       "        same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n",
       "        like `1`) on which the model will be allocated, the device map will map the entire model to this\n",
       "        device. Passing `device_map = 0` means put the whole model on GPU 0.\n",
       "\n",
       "        To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
       "        more information about each option see [designing a device\n",
       "        map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
       "    max_memory (`Dict`, *optional*):\n",
       "        A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
       "        GPU and the available CPU RAM if unset.\n",
       "    offload_folder (`str` or `os.PathLike`, *optional*):\n",
       "        If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
       "    offload_state_dict (`bool`, *optional*):\n",
       "        If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\n",
       "        RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\n",
       "        `True` when there is some disk offload.\n",
       "    load_in_8bit (`bool`, *optional*, defaults to `False`):\n",
       "        If `True`, will convert the loaded model into mixed-8bit quantized model. To use this feature please\n",
       "        install `bitsandbytes` (`pip install -U bitsandbytes`).\n",
       "    load_in_4bit (`bool`, *optional*, defaults to `False`):\n",
       "        If `True`, will convert the loaded model into 4bit precision quantized model. To use this feature\n",
       "        install the latest version of `bitsandbytes` (`pip install -U bitsandbytes`).\n",
       "    quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\n",
       "        A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\n",
       "        bitsandbytes, gptq)\n",
       "    subfolder (`str`, *optional*, defaults to `\"\"`):\n",
       "        In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
       "        specify the folder name here.\n",
       "    variant (`str`, *optional*):\n",
       "        If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is\n",
       "        ignored when using `from_tf` or `from_flax`.\n",
       "    use_safetensors (`bool`, *optional*, defaults to `None`):\n",
       "        Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\n",
       "        is not installed, it will be set to `False`.\n",
       "\n",
       "    kwargs (remaining dictionary of keyword arguments, *optional*):\n",
       "        Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
       "        `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
       "        automatically loaded:\n",
       "\n",
       "            - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
       "              underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
       "              already been done)\n",
       "            - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
       "              initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
       "              corresponds to a configuration attribute will be used to override said attribute with the\n",
       "              supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
       "              will be passed to the underlying model's `__init__` function.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n",
       "use this method in a firewalled environment.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Examples:\n",
       "\n",
       "```python\n",
       ">>> from transformers import BertConfig, BertModel\n",
       "\n",
       ">>> # Download model and configuration from huggingface.co and cache.\n",
       ">>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
       ">>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n",
       ">>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n",
       ">>> # Update configuration during loading.\n",
       ">>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
       ">>> assert model.config.output_attentions == True\n",
       ">>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
       ">>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n",
       ">>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n",
       ">>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n",
       ">>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\n",
       "```\n",
       "\n",
       "* `low_cpu_mem_usage` algorithm:\n",
       "\n",
       "This is an experimental function that loads the model using ~1x model size CPU memory\n",
       "\n",
       "Here is how it works:\n",
       "\n",
       "1. save which state_dict keys we have\n",
       "2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\n",
       "3. after the model has been instantiated switch to the meta device all params/buffers that\n",
       "are going to be replaced from the loaded state_dict\n",
       "4. load state_dict 2nd time\n",
       "5. replace the params/buffers from the state_dict\n",
       "\n",
       "Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\n",
       "\u001b[0;31mFile:\u001b[0m      ~/personal/repos/playground/env/lib/python3.8/site-packages/transformers/modeling_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RobertaModel.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dfa5a74-2606-4bb9-81ec-be49e0ecafa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f357591-cae4-4ab2-b01c-23594193bd4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5c8a60-f71c-41a0-a8dd-c9cac0a1d10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.703061e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.703011e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.703011e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.703008e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  5.703061e+17           neutral                        1.0000   \n",
       "1  5.703011e+17          positive                        0.3486   \n",
       "2  5.703011e+17           neutral                        0.6837   \n",
       "3  5.703010e+17          negative                        1.0000   \n",
       "4  5.703008e+17          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f1fa1-72c4-4a1b-9718-f8104c42c0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd21fcf-4483-48c5-8030-88985d33a81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
