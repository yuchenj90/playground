{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Load spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "with open('data/spambase.names') as f:\n",
    "    line = f.readline()\n",
    "    while (len(line)>0):\n",
    "        line = f.readline()\n",
    "        if (not line.startswith(('|','\\n',' ','1','\\n'))) and (line.find(':')>0):\n",
    "            feature_list.append(line[:line.find(':')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/spambase.data',sep=',', header=None, names=feature_list+['is_spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total      is_spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data to train vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[feature_list])\n",
    "y = np.array(df['is_spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=41235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_log = np.log(X_train+0.1)\n",
    "X_test_log = np.log(X_test+0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchenj/learn/playground/env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression().fit(X_train_log, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05793285055957867"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Error rate\n",
    "y_hat = logreg.predict(X_test_log)\n",
    "(y_hat!=y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate(y_predict, y):\n",
    "    df_res = pd.DataFrame(np.array([y_predict,y_test]).T, columns=['y_predict','y'])\n",
    "    return df_res.groupby(['y_predict','y']).apply(lambda x: x.index.nunique()/len(df_res)).unstack(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_predict</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600395</td>\n",
       "      <td>0.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026333</td>\n",
       "      <td>0.341672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y                 0         1\n",
       "y_predict                    \n",
       "0          0.600395  0.031600\n",
       "1          0.026333  0.341672"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_error_rate(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchenj/learn/playground/env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Weighted loss, less tolerant on misclassify an email to spam (low Type-2 error rate, or False Negative rate.)\n",
    "logreg = LogisticRegression(class_weight={0:5,1:1}).fit(X_train_log, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07834101382488479"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Error rate\n",
    "y_hat = logreg.predict(X_test_log)\n",
    "(y_hat!=y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_predict</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.615537</td>\n",
       "      <td>0.067149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011192</td>\n",
       "      <td>0.306122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y                 0         1\n",
       "y_predict                    \n",
       "0          0.615537  0.067149\n",
       "1          0.011192  0.306122"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_error_rate(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.05, criterion='entropy')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', ccp_alpha=0.05)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5071221533577512"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 1246.0/3082\n",
    "p_hat = 87.0/774\n",
    "-np.log2(p_hat)*p_hat - (1-p_hat)*np.log2(1-p_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "data = tree.export_graphviz(clf, out_file=None, feature_names= feature_list, class_names=['email','spam'],filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.44.0 (20200408.0750)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"444pt\" height=\"433pt\"\n",
       " viewBox=\"0.00 0.00 443.50 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-429 439.5,-429 439.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#f7d7bf\" stroke=\"black\" d=\"M352,-425C352,-425 236,-425 236,-425 230,-425 224,-419 224,-413 224,-413 224,-354 224,-354 224,-348 230,-342 236,-342 236,-342 352,-342 352,-342 358,-342 364,-348 364,-354 364,-354 364,-413 364,-413 364,-419 358,-425 352,-425\"/>\n",
       "<text text-anchor=\"start\" x=\"233.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">char_freq_$ â‰¤ 0.056</text>\n",
       "<text text-anchor=\"start\" x=\"246.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.973</text>\n",
       "<text text-anchor=\"start\" x=\"245.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3082</text>\n",
       "<text text-anchor=\"start\" x=\"232\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1836, 1246]</text>\n",
       "<text text-anchor=\"start\" x=\"254\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = email</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#eda978\" stroke=\"black\" d=\"M288.5,-306C288.5,-306 133.5,-306 133.5,-306 127.5,-306 121.5,-300 121.5,-294 121.5,-294 121.5,-235 121.5,-235 121.5,-229 127.5,-223 133.5,-223 133.5,-223 288.5,-223 288.5,-223 294.5,-223 300.5,-229 300.5,-235 300.5,-235 300.5,-294 300.5,-294 300.5,-300 294.5,-306 288.5,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"129.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_remove â‰¤ 0.055</text>\n",
       "<text text-anchor=\"start\" x=\"163.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.799</text>\n",
       "<text text-anchor=\"start\" x=\"162.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2308</text>\n",
       "<text text-anchor=\"start\" x=\"153\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1749, 559]</text>\n",
       "<text text-anchor=\"start\" x=\"171\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = email</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M265.2,-341.91C258.89,-333.01 252.15,-323.51 245.64,-314.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"248.39,-312.15 239.75,-306.02 242.68,-316.2 248.39,-312.15\"/>\n",
       "<text text-anchor=\"middle\" x=\"235.56\" y=\"-326.97\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#52a9e8\" stroke=\"black\" d=\"M423.5,-298.5C423.5,-298.5 330.5,-298.5 330.5,-298.5 324.5,-298.5 318.5,-292.5 318.5,-286.5 318.5,-286.5 318.5,-242.5 318.5,-242.5 318.5,-236.5 324.5,-230.5 330.5,-230.5 330.5,-230.5 423.5,-230.5 423.5,-230.5 429.5,-230.5 435.5,-236.5 435.5,-242.5 435.5,-242.5 435.5,-286.5 435.5,-286.5 435.5,-292.5 429.5,-298.5 423.5,-298.5\"/>\n",
       "<text text-anchor=\"start\" x=\"329.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.507</text>\n",
       "<text text-anchor=\"start\" x=\"332\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 774</text>\n",
       "<text text-anchor=\"start\" x=\"326.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [87, 687]</text>\n",
       "<text text-anchor=\"start\" x=\"336.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = spam</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>0&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M322.8,-341.91C330.78,-330.65 339.46,-318.42 347.48,-307.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.54,-308.85 353.47,-298.67 344.83,-304.8 350.54,-308.85\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.66\" y=\"-319.61\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ea9a61\" stroke=\"black\" d=\"M194,-187C194,-187 84,-187 84,-187 78,-187 72,-181 72,-175 72,-175 72,-116 72,-116 72,-110 78,-104 84,-104 84,-104 194,-104 194,-104 200,-104 206,-110 206,-116 206,-116 206,-175 206,-175 206,-181 200,-187 194,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"80\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">char_freq_! â‰¤ 0.232</text>\n",
       "<text text-anchor=\"start\" x=\"91.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.653</text>\n",
       "<text text-anchor=\"start\" x=\"90.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2083</text>\n",
       "<text text-anchor=\"start\" x=\"81\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1733, 350]</text>\n",
       "<text text-anchor=\"start\" x=\"99\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = email</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186.02,-222.91C180.6,-214.1 174.82,-204.7 169.22,-195.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"172.16,-193.7 163.94,-187.02 166.2,-197.37 172.16,-193.7\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#48a5e7\" stroke=\"black\" d=\"M329.5,-179.5C329.5,-179.5 236.5,-179.5 236.5,-179.5 230.5,-179.5 224.5,-173.5 224.5,-167.5 224.5,-167.5 224.5,-123.5 224.5,-123.5 224.5,-117.5 230.5,-111.5 236.5,-111.5 236.5,-111.5 329.5,-111.5 329.5,-111.5 335.5,-111.5 341.5,-117.5 341.5,-123.5 341.5,-123.5 341.5,-167.5 341.5,-167.5 341.5,-173.5 335.5,-179.5 329.5,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"239\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.37</text>\n",
       "<text text-anchor=\"start\" x=\"238\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 225</text>\n",
       "<text text-anchor=\"start\" x=\"232.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [16, 209]</text>\n",
       "<text text-anchor=\"start\" x=\"242.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = spam</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M235.98,-222.91C242.84,-211.76 250.29,-199.66 257.19,-188.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.33,-190.02 262.59,-179.67 254.37,-186.35 260.33,-190.02\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#e78d4c\" stroke=\"black\" d=\"M120,-68C120,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 120,0 120,0 126,0 132,-6 132,-12 132,-12 132,-56 132,-56 132,-62 126,-68 120,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"18.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.428</text>\n",
       "<text text-anchor=\"start\" x=\"17.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1704</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1555, 149]</text>\n",
       "<text text-anchor=\"start\" x=\"26\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = email</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M111.82,-103.73C105.92,-94.88 99.67,-85.51 93.75,-76.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.66,-74.68 88.2,-68.3 90.83,-78.56 96.66,-74.68\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#e8f4fc\" stroke=\"black\" d=\"M263.5,-68C263.5,-68 162.5,-68 162.5,-68 156.5,-68 150.5,-62 150.5,-56 150.5,-56 150.5,-12 150.5,-12 150.5,-6 156.5,0 162.5,0 162.5,0 263.5,0 263.5,0 269.5,0 275.5,-6 275.5,-12 275.5,-12 275.5,-56 275.5,-56 275.5,-62 269.5,-68 263.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"165.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.997</text>\n",
       "<text text-anchor=\"start\" x=\"168\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 379</text>\n",
       "<text text-anchor=\"start\" x=\"158.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [178, 201]</text>\n",
       "<text text-anchor=\"start\" x=\"172.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = spam</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.55,-103.73C172.53,-94.88 178.87,-85.51 184.87,-76.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"187.8,-78.54 190.5,-68.3 182,-74.63 187.8,-78.54\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x12d505dd0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from IPython.display import display\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(criterion, max_leaf_nodes):\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier(criterion=criterion, max_leaf_nodes=max_leaf_nodes)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    graph = graphviz.Source(tree.export_graphviz(clf, \n",
    "                                                 out_file=None, \n",
    "                                                 feature_names= feature_list, \n",
    "                                                 class_names=['email','spam'],\n",
    "                                                 filled=True, \n",
    "                                                 rounded=True, \n",
    "                                                 special_characters=True)) \n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "\n",
    "inter=interactive(plot_tree \n",
    "                  , criterion = [\"gini\", \"entropy\"]\n",
    "                  , max_leaf_nodes=[i for i in range(2,21)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ee43264eb64821bb879fae9027a340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='criterion', options=('gini', 'entropy'), value='gini'), Dropdown(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def f(a, b):\n",
    "    display(a + b)\n",
    "    return a+b\n",
    "\n",
    "w = interactive(f, a=10, b=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3082"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3338404694631602"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1536.0/(1536+3065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd        \n",
    "from pygam import LogisticGAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLogisticGAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mterms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deviance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'diffs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Logistic GAM\n",
       "\n",
       "This is a GAM with a Binomial error distribution, and a logit link.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "terms : expression specifying terms to model, optional.\n",
       "\n",
       "    By default a univariate spline term will be allocated for each feature.\n",
       "\n",
       "    For example:\n",
       "\n",
       "    >>> GAM(s(0) + l(1) + f(2) + te(3, 4))\n",
       "\n",
       "    will fit a spline term on feature 0, a linear term on feature 1,\n",
       "    a factor term on feature 2, and a tensor term on features 3 and 4.\n",
       "\n",
       "callbacks : list of str or list of CallBack objects, optional\n",
       "    Names of callback objects to call during the optimization loop.\n",
       "\n",
       "fit_intercept : bool, optional\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "    Note: the intercept receives no smoothing penalty.\n",
       "\n",
       "max_iter : int, optional\n",
       "    Maximum number of iterations allowed for the solver to converge.\n",
       "\n",
       "tol : float, optional\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "verbose : bool, optional\n",
       "    whether to show pyGAM warnings.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : array, shape (n_classes, m_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "    If fit_intercept is True, then self.coef_[0] will contain the bias.\n",
       "\n",
       "statistics_ : dict\n",
       "    Dictionary containing model statistics like GCV/UBRE scores, AIC/c,\n",
       "    parameter covariances, estimated degrees of freedom, etc.\n",
       "\n",
       "logs_ : dict\n",
       "    Dictionary containing the outputs of any callbacks at each\n",
       "    optimization loop.\n",
       "\n",
       "    The logs are structured as ``{callback: [...]}``\n",
       "\n",
       "References\n",
       "----------\n",
       "Simon N. Wood, 2006\n",
       "Generalized Additive Models: an introduction with R\n",
       "\n",
       "Hastie, Tibshirani, Friedman\n",
       "The Elements of Statistical Learning\n",
       "http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf\n",
       "\n",
       "Paul Eilers & Brian Marx, 2015\n",
       "International Biometric Society: A Crash Course on P-splines\n",
       "http://www.ibschannel2015.nl/project/userfiles/Crash_course_handout.pdf\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "creates an instance of the Core class\n",
       "\n",
       "comes loaded with useful methods\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "name : str, default: None\n",
       "line_width : int, default: 70\n",
       "    number of characters to print on a line\n",
       "line_offset : int, default: 3\n",
       "    number of characters to indent after the first line\n",
       "\n",
       "Returns\n",
       "-------\n",
       "self\n",
       "\u001b[0;31mFile:\u001b[0m           ~/learn/playground/env/lib/python3.7/site-packages/pygam/pygam.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LogisticGAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  \n",
       "count       569.000000        569.000000  \n",
       "mean          0.096360          0.104341  \n",
       "std           0.014064          0.052813  \n",
       "min           0.052630          0.019380  \n",
       "25%           0.086370          0.064920  \n",
       "50%           0.095870          0.092630  \n",
       "75%           0.105300          0.130400  \n",
       "max           0.163400          0.345400  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd        \n",
    "from pygam import LogisticGAM, GAM, l, s, f\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "#load the breast cancer data set\n",
    "data = load_breast_cancer()\n",
    "#keep first 6 features only\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)[['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness', 'mean compactness']]\n",
    "target_df = pd.Series(data.target)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness', 'mean compactness']]\n",
    "y = target_df\n",
    "#Fit a model with the default parameters\n",
    "gam = LogisticGAM(l(0)+l(1)+l(2)+l(3)+l(4)+l(5)).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticGAM                                                                                               \n",
      "=============================================== ==========================================================\n",
      "Distribution:                      BinomialDist Effective DoF:                                      4.4857\n",
      "Link Function:                        LogitLink Log Likelihood:                                   -110.929\n",
      "Number of Samples:                          569 AIC:                                              230.8293\n",
      "                                                AICc:                                             230.9558\n",
      "                                                UBRE:                                                2.412\n",
      "                                                Scale:                                                 1.0\n",
      "                                                Pseudo R-Squared:                                   0.7048\n",
      "==========================================================================================================\n",
      "Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n",
      "================================= ==================== ============ ============ ============ ============\n",
      "l(0)                              [0.6]                1            1.0          1.44e-09     ***         \n",
      "l(1)                              [0.6]                1            1.0          1.09e-07     ***         \n",
      "l(2)                              [0.6]                1            0.8          1.95e-12     ***         \n",
      "l(3)                              [0.6]                1            1.0          9.15e-01                 \n",
      "l(4)                              [0.6]                1            0.0          1.22e-13     ***         \n",
      "l(5)                              [0.6]                1            0.0          1.03e-10     ***         \n",
      "intercept                                              1            0.7          1.49e-03     **          \n",
      "==========================================================================================================\n",
      "Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n",
      "         which can cause p-values to appear significant when they are not.\n",
      "\n",
      "WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n",
      "         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n",
      "         are typically lower than they should be, meaning that the tests reject the null too readily.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchenj/learn/playground/env/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n",
      " \n",
      "Please do not make inferences based on these values! \n",
      "\n",
      "Collaborate on a solution, and stay up to date at: \n",
      "github.com/dswah/pyGAM/issues/163 \n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "gam.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9138840070298769"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gam.accuracy(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.82448051e+00, -2.25317749e-01, -7.18556925e-01, -8.49193383e-04,\n",
       "       -7.83779522e-01, -1.19868359e+00,  1.77725470e+01])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gam.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchenj/learn/playground/env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lgreg = LogisticRegression().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.93218947, -0.22367123, -0.65404897,  0.0040385 , -0.50285347,\n",
       "        -0.82435899]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Logistic Regression (aka logit, MaxEnt) classifier.\n",
       "\n",
       "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
       "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
       "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
       "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
       "'sag', 'saga' and 'newton-cg' solvers.)\n",
       "\n",
       "This class implements regularized logistic regression using the\n",
       "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
       "that regularization is applied by default**. It can handle both dense\n",
       "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
       "floats for optimal performance; any other input format will be converted\n",
       "(and copied).\n",
       "\n",
       "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
       "with primal formulation, or no regularization. The 'liblinear' solver\n",
       "supports both L1 and L2 regularization, with a dual formulation only for\n",
       "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
       "'saga' solver.\n",
       "\n",
       "Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
       "    Used to specify the norm used in the penalization. The 'newton-cg',\n",
       "    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
       "    only supported by the 'saga' solver. If 'none' (not supported by the\n",
       "    liblinear solver), no regularization is applied.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
       "\n",
       "dual : bool, default=False\n",
       "    Dual or primal formulation. Dual formulation is only implemented for\n",
       "    l2 penalty with liblinear solver. Prefer dual=False when\n",
       "    n_samples > n_features.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "C : float, default=1.0\n",
       "    Inverse of regularization strength; must be a positive float.\n",
       "    Like in support vector machines, smaller values specify stronger\n",
       "    regularization.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    Useful only when the solver 'liblinear' is used\n",
       "    and self.fit_intercept is set to True. In this case, x becomes\n",
       "    [x, self.intercept_scaling],\n",
       "    i.e. a \"synthetic\" feature with constant value equal to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *class_weight='balanced'*\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
       "    data. See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
       "\n",
       "    Algorithm to use in the optimization problem.\n",
       "\n",
       "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
       "      'saga' are faster for large ones.\n",
       "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
       "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
       "      schemes.\n",
       "    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
       "    - 'liblinear' and 'saga' also handle L1 penalty\n",
       "    - 'saga' also supports 'elasticnet' penalty\n",
       "    - 'liblinear' does not support setting ``penalty='none'``\n",
       "\n",
       "    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
       "    features with approximately the same scale. You can\n",
       "    preprocess the data with a scaler from sklearn.preprocessing.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "    .. versionchanged:: 0.22\n",
       "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
       "\n",
       "max_iter : int, default=100\n",
       "    Maximum number of iterations taken for the solvers to converge.\n",
       "\n",
       "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
       "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
       "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
       "    across the entire probability distribution, *even when the data is\n",
       "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
       "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
       "    and otherwise selects 'multinomial'.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "    .. versionchanged:: 0.22\n",
       "        Default changed from 'ovr' to 'auto' in 0.22.\n",
       "\n",
       "verbose : int, default=0\n",
       "    For the liblinear and lbfgs solvers set verbose to any positive\n",
       "    number for verbosity.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of CPU cores used when parallelizing over classes if\n",
       "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
       "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
       "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors.\n",
       "    See :term:`Glossary <n_jobs>` for more details.\n",
       "\n",
       "l1_ratio : float, default=None\n",
       "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
       "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
       "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
       "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "\n",
       "classes_ : ndarray of shape (n_classes, )\n",
       "    A list of class labels known to the classifier.\n",
       "\n",
       "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "\n",
       "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
       "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
       "\n",
       "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
       "    Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "    `intercept_` is of shape (1,) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
       "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
       "    outcome 0 (False).\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
       "    Actual number of iterations for all classes. If binary or multinomial,\n",
       "    it returns only 1 element. For liblinear solver, only the maximum\n",
       "    number of iteration across all classes is given.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "\n",
       "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
       "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SGDClassifier : Incrementally trained logistic regression (when given\n",
       "    the parameter ``loss=\"log\"``).\n",
       "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The underlying C implementation uses a random number generator to\n",
       "select features when fitting the model. It is thus not uncommon,\n",
       "to have slightly different results for the same input data. If\n",
       "that happens, try with a smaller tol parameter.\n",
       "\n",
       "Predict output may not match that of standalone liblinear in certain\n",
       "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "in the narrative documentation.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
       "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
       "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
       "\n",
       "LIBLINEAR -- A Library for Large Linear Classification\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
       "\n",
       "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
       "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
       "    https://hal.inria.fr/hal-00860051/document\n",
       "\n",
       "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
       "    SAGA: A Fast Incremental Gradient Method With Support\n",
       "    for Non-Strongly Convex Composite Objectives\n",
       "    https://arxiv.org/abs/1407.0202\n",
       "\n",
       "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
       "    methods for logistic regression and maximum entropy models.\n",
       "    Machine Learning 85(1-2):41-75.\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.linear_model import LogisticRegression\n",
       ">>> X, y = load_iris(return_X_y=True)\n",
       ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
       ">>> clf.predict(X[:2, :])\n",
       "array([0, 0])\n",
       ">>> clf.predict_proba(X[:2, :])\n",
       "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
       "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
       ">>> clf.score(X, y)\n",
       "0.97...\n",
       "\u001b[0;31mFile:\u001b[0m           ~/learn/playground/env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     LogisticRegressionCV\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
