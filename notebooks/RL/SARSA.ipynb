{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from environments import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(qlist, epsilon):\n",
    "    '''\n",
    "        Choose an action based on q(s,*) values using epsilon greedy\n",
    "    '''\n",
    "    num = np.random.uniform(0,1,1)\n",
    "    num_actions = len(qlist)\n",
    "    best_action = np.argmax(qlist)\n",
    "\n",
    "    if num >= num_actions*epsilon:\n",
    "        return best_action\n",
    "    else:\n",
    "        return int(num/epsilon)\n",
    "\n",
    "def SARSA(env, alpha, gamma=1, epsilon = 0.1, num_iters = 100, max_steps=1000, random_seed=None):\n",
    "    '''\n",
    "        Implement SARSA on-policy alg using epsilon-greedy as both behavior and target policy\n",
    "        Q(s,a) <-- Q(s,a) + alpha * (R + gamma * Q(s_next, a_next) - Q(s,a))\n",
    "        params:\n",
    "            env: environment\n",
    "            alpha: learning rate\n",
    "            gamma: discount factor\n",
    "            epsilon: probability used in generating epsilon-greedy algs\n",
    "            num_iters: number of iterations for training\n",
    "            max_steps: maximum number of steps for each policy rollout\n",
    "    '''\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    q = np.zeros((len(env.state_dict),len(env.action_list)))\n",
    "    for i in range(num_iters):\n",
    "        env.reset()\n",
    "        state = env.state\n",
    "        state_index = env.state_dict[state]\n",
    "        action_index = epsilon_greedy(q[state_index], epsilon)\n",
    "        action = env.action_list[action_index]\n",
    "        num_steps = 0\n",
    "        \n",
    "        while (not env.terminated) and (num_steps<max_steps):\n",
    "            reward = env.step(action)\n",
    "            num_steps += 1\n",
    "            \n",
    "            s_next = env.state\n",
    "            s_next_index = env.state_dict[s_next]\n",
    "            a_next_index = epsilon_greedy(q[s_next_index], epsilon)      \n",
    "            a_next = env.action_list[a_next_index]\n",
    "\n",
    "            q[state_index, action_index] = q[state_index, action_index] + alpha*(reward + gamma*q[s_next_index, a_next_index] - q[state_index, action_index])\n",
    "\n",
    "            state = s_next\n",
    "            state_index = s_next_index\n",
    "            action = a_next\n",
    "            action_index = a_next_index\n",
    "            \n",
    "    # get policy from best q\n",
    "    policy = {}\n",
    "    for state in env.state_dict.keys():\n",
    "        policy[state] = np.zeros(len(env.action_list)) + 0.05\n",
    "        policy[state][np.argmax(q[env.state_dict[state]])] = 1 - 0.05*(len(env.action_list)-1)\n",
    "\n",
    "    return q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(env,policy, num_iter = 100, max_steps=1000, random_seed=None):\n",
    "    '''\n",
    "        Evaluate a given policy, where policy[s] is a probability distribution over p(a|s), using an average over num_iter rollouts\n",
    "    '''\n",
    "    rewards_list = []\n",
    "    steps_list = []\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        env.reset()\n",
    "        state = env.state\n",
    "        num_steps = 0\n",
    "        rewards = 0\n",
    "        while (not env.terminated) and (num_steps<max_steps):\n",
    "            action_index = np.random.choice([i for i in range(len(policy[state]))], 1, p=policy[state])[0]\n",
    "            action = env.action_list[action_index]\n",
    "\n",
    "            r = env.step(action)\n",
    "            state = env.state\n",
    "            #print(state, r)\n",
    "            num_steps += 1\n",
    "            rewards += r\n",
    "        \n",
    "        rewards_list.append(rewards)\n",
    "        steps_list.append(num_steps)\n",
    "    \n",
    "    r = np.array(rewards_list)\n",
    "    \n",
    "    print('mean rewards:', r.mean())\n",
    "    print('std:', r.std())\n",
    "    print('max rewards:', r.max())\n",
    "    print('min rewards:', r.min())\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlearning(env, alpha, gamma=1, nsteps=1, epsilon = 0.1, num_iters = 100, max_steps=1000, random_seed=None):\n",
    "    '''\n",
    "        Implement Q-learning, off-policy alg using epsilon-greedy as behavior policy and deterministic policy (based on Q values) as target policy\n",
    "        Q(s,a) <-- Q(s,a) + alpha * (R + gamma * max_a{Q(s_next, a)} - Q(s,a))\n",
    "        params:\n",
    "            env: environment\n",
    "            alpha: learning rate\n",
    "            gamma: discount factor\n",
    "            epsilon: probability used in generating epsilon-greedy algs\n",
    "            num_iters: number of iterations for training\n",
    "            max_steps: maximum number of steps for each policy rollout\n",
    "    '''\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    q = np.zeros((len(env.state_dict),len(env.action_list)))   \n",
    "    for i in range(num_iters):\n",
    "        env.reset()\n",
    "        state = env.state\n",
    "        state_index = env.state_dict[state]\n",
    "        num_steps = 0\n",
    "        \n",
    "        while (not env.terminated) and (num_steps<max_steps):\n",
    "            action_index = epsilon_greedy(q[state_index], epsilon)\n",
    "            action = env.action_list[action_index]\n",
    "            reward = env.step(action)\n",
    "            num_steps += 1\n",
    "            \n",
    "            s_next = env.state\n",
    "            s_next_index = env.state_dict[s_next]\n",
    "\n",
    "            q[state_index, action_index] = q[state_index, action_index] + alpha*(reward + gamma*q[s_next_index].max() - q[state_index, action_index])\n",
    "\n",
    "            state = s_next\n",
    "            state_index = s_next_index\n",
    "            \n",
    "    # get policy from best q\n",
    "    policy = {}\n",
    "    for state in env.state_dict.keys():\n",
    "        policy[state] = np.zeros(len(env.action_list)) + 0.05\n",
    "        policy[state][np.argmax(q[env.state_dict[state]])] = 1 - 0.05*(len(env.action_list)-1)\n",
    "\n",
    "    return q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WindyGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = 10\n",
    "ymax = 7\n",
    "upwind = [0,0,0,1,1,1,2,2,1,0]\n",
    "\n",
    "grid_wind = {}\n",
    "for x in range(xmax):\n",
    "    for y in range(ymax):\n",
    "        grid_wind[(x,y)] = upwind[x]\n",
    "\n",
    "start_state = (0,3)\n",
    "goal_state = (7,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = windy_gridworld(grid_wind, goal_state, start_state, prob=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "[q,policy] = SARSA(env,alpha=0.5,num_iters=10000,random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {}\n",
    "for state in policy:\n",
    "    p[state] = env.action_list[np.argmax(policy[state])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-31.14742522, -31.9457328 , -27.82297286, -32.35990984])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[env.state_dict[(3,5)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n",
      "(1, 3)\n",
      "(2, 3)\n",
      "(3, 3)\n",
      "(4, 4)\n",
      "(5, 5)\n",
      "(6, 6)\n",
      "(7, 6)\n",
      "(8, 6)\n",
      "(9, 6)\n",
      "(9, 5)\n",
      "(9, 4)\n",
      "(9, 3)\n",
      "(9, 2)\n",
      "(8, 2)\n",
      "-15\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = 0\n",
    "\n",
    "while not env.terminated:\n",
    "    print(env.state)\n",
    "    action = p[env.state]\n",
    "    r = env.step(action)\n",
    "    rewards += r\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean rewards: -20.49\n",
      "std: 5.828370269637989\n",
      "max rewards: -15\n",
      "min rewards: -46\n"
     ]
    }
   ],
   "source": [
    "r = eval(env, policy, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean rewards: -19.91\n",
      "std: 4.310672801315359\n",
      "max rewards: -15\n",
      "min rewards: -35\n"
     ]
    }
   ],
   "source": [
    "[q, policy] = Qlearning(env, alpha=0.5, num_iters=8000, random_seed=1)\n",
    "r = eval(env, policy, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean rewards: -116.16\n",
      "std: 53.930088077065115\n",
      "max rewards: -30\n",
      "min rewards: -325\n"
     ]
    }
   ],
   "source": [
    "[q, policy] = SARSA(env, alpha=0.5, num_iters=8000, random_seed=1)\n",
    "r = eval(env, policy, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = windy_gridworld(grid_wind, goal_state, start_state, prob=1.0/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "[q, policy] = Qlearning(env, alpha=0.5, num_iters=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean rewards: -73.44\n",
      "std: 57.46169506723587\n",
      "max rewards: -8\n",
      "min rewards: -291\n"
     ]
    }
   ],
   "source": [
    "r = eval(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (0, 1): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (0, 2): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (0, 3): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (0, 4): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (0, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (0, 6): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (1, 0): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (1, 1): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (1, 2): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (1, 3): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (1, 4): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (1, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (1, 6): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (2, 0): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (2, 1): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (2, 2): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (2, 3): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (2, 4): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (2, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (2, 6): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (3, 0): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (3, 1): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (3, 2): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (3, 3): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (3, 4): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (3, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (3, 6): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (4, 0): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (4, 1): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (4, 2): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (4, 3): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (4, 4): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (4, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (4, 6): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (5, 0): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (5, 1): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (5, 2): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (5, 3): array([0.85, 0.05, 0.05, 0.05]),\n",
       " (5, 4): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (5, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (5, 6): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (6, 0): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (6, 1): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (6, 2): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (6, 3): array([0.85, 0.05, 0.05, 0.05]),\n",
       " (6, 4): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (6, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (6, 6): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (7, 0): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (7, 1): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (7, 2): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (7, 3): array([0.85, 0.05, 0.05, 0.05]),\n",
       " (7, 4): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (7, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (7, 6): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (8, 0): array([0.05, 0.05, 0.05, 0.85]),\n",
       " (8, 1): array([0.05, 0.05, 0.05, 0.85]),\n",
       " (8, 2): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (8, 3): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (8, 4): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (8, 5): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (8, 6): array([0.05, 0.05, 0.85, 0.05]),\n",
       " (9, 0): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (9, 1): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (9, 2): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (9, 3): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (9, 4): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (9, 5): array([0.05, 0.85, 0.05, 0.05]),\n",
       " (9, 6): array([0.05, 0.85, 0.05, 0.05])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "[q,policy] = SARSA(env, alpha=0.5, num_iters=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean rewards: -45.0\n",
      "std: 28.852382917187274\n",
      "max rewards: -11\n",
      "min rewards: -233\n"
     ]
    }
   ],
   "source": [
    "r = eval(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = cliff_walking(size=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "[q,policy] = SARSA(env, alpha=0.5, num_iters=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {}\n",
    "for state in policy:\n",
    "    p[state] = env.action_list[np.argmax(policy[state])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 'up',\n",
       " (0, 1): 'up',\n",
       " (0, 2): 'up',\n",
       " (0, 3): 'right',\n",
       " (1, 0): 'up',\n",
       " (1, 1): 'up',\n",
       " (1, 2): 'right',\n",
       " (1, 3): 'right',\n",
       " (2, 0): 'up',\n",
       " (2, 1): 'left',\n",
       " (2, 2): 'up',\n",
       " (2, 3): 'up',\n",
       " (3, 0): 'up',\n",
       " (3, 1): 'up',\n",
       " (3, 2): 'up',\n",
       " (3, 3): 'right',\n",
       " (4, 0): 'up',\n",
       " (4, 1): 'left',\n",
       " (4, 2): 'right',\n",
       " (4, 3): 'right',\n",
       " (5, 0): 'up',\n",
       " (5, 1): 'up',\n",
       " (5, 2): 'right',\n",
       " (5, 3): 'right',\n",
       " (6, 0): 'up',\n",
       " (6, 1): 'up',\n",
       " (6, 2): 'up',\n",
       " (6, 3): 'right',\n",
       " (7, 0): 'up',\n",
       " (7, 1): 'up',\n",
       " (7, 2): 'right',\n",
       " (7, 3): 'right',\n",
       " (8, 0): 'up',\n",
       " (8, 1): 'up',\n",
       " (8, 2): 'right',\n",
       " (8, 3): 'right',\n",
       " (9, 0): 'up',\n",
       " (9, 1): 'up',\n",
       " (9, 2): 'up',\n",
       " (9, 3): 'right',\n",
       " (10, 0): 'up',\n",
       " (10, 1): 'right',\n",
       " (10, 2): 'right',\n",
       " (10, 3): 'down',\n",
       " (11, 0): 'up',\n",
       " (11, 1): 'down',\n",
       " (11, 2): 'down',\n",
       " (11, 3): 'down'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
