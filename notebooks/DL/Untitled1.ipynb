{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bb566e-ec8b-4ab0-a9fc-001749790781",
   "metadata": {},
   "source": [
    "Train a NN to predict y = sin(x1x2+x3^x4-x5)>cos(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc090586-c20a-4e8e-995d-302ab00b7f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f2fe8-77f5-4d82-a7dc-1a73a3b23ac9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# data sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7d1b99b1-de6f-4299-9499-fc49b874656b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = 1000\n",
    "n = 10000\n",
    "np.random.seed(12)\n",
    "x = np.random.uniform(size=(n,m))\n",
    "y = np.sin((x[:,1]*x[:,2]) + np.power(x[:,3], x[:,4]) - x[:,5]) > np.cos(x[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a571b4f6-96fa-4d97-879a-c380d0182860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1312"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5a3ee-baba-426a-8ce0-2ddc714514ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9ffefb-2197-4749-8248-25b7cb320347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c354c93a-f751-4092-8479-d238960f9337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test,  y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8ecf97-6276-4a46-bdf1-2bb29d4ac803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac7f4e-0099-4c1f-a368-41403ee49a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1c5fea8-57b1-4870-b37a-84b1c7c20d31",
   "metadata": {},
   "source": [
    "# Model class for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f6fb64-63de-49bc-9762-4acc84a902bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, hidden_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.layers.append((f'conv{i}', nn.Linear(hidden_dims[i], hidden_dims[i+1])))\n",
    "            #self.layers.append((f'BatchNorm{i}', nn.BatchNorm1d(hidden_dims[i+1])))\n",
    "            #self.layers.append((f'dropout{i}', nn.Dropout(p=0.1)))\n",
    "            self.layers.append((f'relu{i}', nn.ReLU()))\n",
    "        self.layers.append(('output', nn.Linear(hidden_dims[-1], 1)))\n",
    "        print(self.layers)\n",
    "        \n",
    "    def build(self) -> None:\n",
    "        '''\n",
    "        def init_weights(m):\n",
    "            if type(m)==nn.Linear:\n",
    "                nn.init.normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        '''\n",
    "        self.model = nn.Sequential(OrderedDict(self.layers))\n",
    "        # self.model.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99fc974a-bf95-4c5a-9802-516ea99c3cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conv0', Linear(in_features=1000, out_features=512, bias=True)), ('relu0', ReLU()), ('conv1', Linear(in_features=512, out_features=128, bias=True)), ('relu1', ReLU()), ('conv2', Linear(in_features=128, out_features=64, bias=True)), ('relu2', ReLU()), ('output', Linear(in_features=64, out_features=1, bias=True))]\n"
     ]
    }
   ],
   "source": [
    "d = DeepNN([1000, 512, 128, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a636ae2-52c6-4601-8293-ecdee69c872e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e25264-8360-462b-9115-ff7080114565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96615647, 0.69441522, 0.36463435, ..., 0.88372571, 0.32812163,\n",
       "        0.65418378],\n",
       "       [0.99086928, 0.35123739, 0.44418384, ..., 0.95551566, 0.72651902,\n",
       "        0.50276685]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a64ac1a-c1d9-4ba6-9775-f4a2f97440cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4cbec98-abef-4d23-aa5d-e4b581f71c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0839],\n",
       "        [0.0746],\n",
       "        [0.0732],\n",
       "        ...,\n",
       "        [0.0710],\n",
       "        [0.0732],\n",
       "        [0.0820]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5acc41f1-79c6-4f6e-b71c-a3e01b6a622f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "input = torch.randn(1, 2)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70656b2a-0ffb-4fe1-84c8-abb7a3a86bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3643, -1.1865]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d4f69bf-0680-4b80-b5c4-f464ac2ca607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "optimizer = torch.optim.Adam(d.parameters())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ccc529ba-f6c6-4122-acc8-105f5d04d345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3b2eeda-4afd-41f6-9f7d-5e5ad7ee71a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0839, 0.0746, 0.0732,  ..., 0.0710, 0.0732, 0.0820],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11c2b28c-1d22-4197-9087-f704e5759c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True,  ...,  True,  True,  True])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fdee1ccc-11e2-46a3-a05e-0d8680dee4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86925"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_pred>0.5).squeeze() == y_train_tensor).numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0283a759-816c-452a-b443-eb7d8539771c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10536, train accuracy: 0.94238\n",
      "Test loss: 0.2336, test accuracy: 0.903\n",
      "Train loss: 0.1105, train accuracy: 0.96788\n",
      "Test loss: 0.30444, test accuracy: 0.8915\n",
      "Train loss: 0.11777, train accuracy: 0.933\n",
      "Test loss: 0.22796, test accuracy: 0.904\n",
      "Train loss: 0.09976, train accuracy: 0.97112\n",
      "Test loss: 0.25071, test accuracy: 0.903\n",
      "Train loss: 0.0884, train accuracy: 0.95538\n",
      "Test loss: 0.23273, test accuracy: 0.9085\n",
      "Train loss: 0.08304, train accuracy: 0.966\n",
      "Test loss: 0.2247, test accuracy: 0.9075\n",
      "Train loss: 0.08619, train accuracy: 0.97238\n",
      "Test loss: 0.27228, test accuracy: 0.9005\n",
      "Train loss: 0.09543, train accuracy: 0.94875\n",
      "Test loss: 0.22951, test accuracy: 0.9055\n",
      "Train loss: 0.10097, train accuracy: 0.97175\n",
      "Test loss: 0.31084, test accuracy: 0.893\n",
      "Train loss: 0.11547, train accuracy: 0.93375\n",
      "Test loss: 0.23529, test accuracy: 0.903\n",
      "Train loss: 0.10909, train accuracy: 0.9705\n",
      "Test loss: 0.31103, test accuracy: 0.895\n",
      "Train loss: 0.11388, train accuracy: 0.93425\n",
      "Test loss: 0.22616, test accuracy: 0.907\n",
      "Train loss: 0.09396, train accuracy: 0.97375\n",
      "Test loss: 0.25309, test accuracy: 0.904\n",
      "Train loss: 0.08206, train accuracy: 0.95762\n",
      "Test loss: 0.22887, test accuracy: 0.907\n",
      "Train loss: 0.07521, train accuracy: 0.97025\n",
      "Test loss: 0.22209, test accuracy: 0.909\n",
      "Train loss: 0.07808, train accuracy: 0.9755\n",
      "Test loss: 0.27109, test accuracy: 0.9025\n",
      "Train loss: 0.0875, train accuracy: 0.952\n",
      "Test loss: 0.22624, test accuracy: 0.9055\n",
      "Train loss: 0.09227, train accuracy: 0.976\n",
      "Test loss: 0.30459, test accuracy: 0.899\n",
      "Train loss: 0.10402, train accuracy: 0.93912\n",
      "Test loss: 0.23045, test accuracy: 0.903\n",
      "Train loss: 0.09833, train accuracy: 0.97462\n",
      "Test loss: 0.30326, test accuracy: 0.899\n",
      "Train loss: 0.10162, train accuracy: 0.94025\n",
      "Test loss: 0.22363, test accuracy: 0.907\n",
      "Train loss: 0.08603, train accuracy: 0.978\n",
      "Test loss: 0.25657, test accuracy: 0.9055\n",
      "Train loss: 0.07625, train accuracy: 0.96088\n",
      "Test loss: 0.22402, test accuracy: 0.9115\n",
      "Train loss: 0.06831, train accuracy: 0.97575\n",
      "Test loss: 0.22198, test accuracy: 0.9115\n",
      "Train loss: 0.06834, train accuracy: 0.97775\n",
      "Test loss: 0.25744, test accuracy: 0.905\n",
      "Train loss: 0.07428, train accuracy: 0.96138\n",
      "Test loss: 0.22124, test accuracy: 0.909\n",
      "Train loss: 0.07931, train accuracy: 0.9805\n",
      "Test loss: 0.29107, test accuracy: 0.9035\n",
      "Train loss: 0.08925, train accuracy: 0.947\n",
      "Test loss: 0.22778, test accuracy: 0.903\n",
      "Train loss: 0.08992, train accuracy: 0.97725\n",
      "Test loss: 0.31725, test accuracy: 0.899\n",
      "Train loss: 0.10258, train accuracy: 0.93938\n",
      "Test loss: 0.23268, test accuracy: 0.9015\n",
      "Train loss: 0.09596, train accuracy: 0.9755\n",
      "Test loss: 0.31914, test accuracy: 0.899\n",
      "Train loss: 0.10202, train accuracy: 0.93912\n",
      "Test loss: 0.22372, test accuracy: 0.9035\n",
      "Train loss: 0.08186, train accuracy: 0.98025\n",
      "Test loss: 0.25766, test accuracy: 0.908\n",
      "Train loss: 0.06889, train accuracy: 0.96262\n",
      "Test loss: 0.22492, test accuracy: 0.9135\n",
      "Train loss: 0.05991, train accuracy: 0.97762\n",
      "Test loss: 0.21822, test accuracy: 0.912\n",
      "Train loss: 0.06359, train accuracy: 0.9835\n",
      "Test loss: 0.27643, test accuracy: 0.904\n",
      "Train loss: 0.07485, train accuracy: 0.95675\n",
      "Test loss: 0.22323, test accuracy: 0.903\n",
      "Train loss: 0.07836, train accuracy: 0.98188\n",
      "Test loss: 0.3003, test accuracy: 0.904\n",
      "Train loss: 0.08578, train accuracy: 0.94788\n",
      "Test loss: 0.2234, test accuracy: 0.9025\n",
      "Train loss: 0.07748, train accuracy: 0.98288\n",
      "Test loss: 0.28003, test accuracy: 0.904\n",
      "Train loss: 0.07366, train accuracy: 0.957\n",
      "Test loss: 0.21757, test accuracy: 0.913\n",
      "Train loss: 0.0618, train accuracy: 0.98562\n",
      "Test loss: 0.23515, test accuracy: 0.9125\n",
      "Train loss: 0.05488, train accuracy: 0.97612\n",
      "Test loss: 0.2329, test accuracy: 0.914\n",
      "Train loss: 0.05374, train accuracy: 0.97738\n",
      "Test loss: 0.2177, test accuracy: 0.913\n",
      "Train loss: 0.0573, train accuracy: 0.98625\n",
      "Test loss: 0.26643, test accuracy: 0.9085\n",
      "Train loss: 0.06345, train accuracy: 0.96362\n",
      "Test loss: 0.22006, test accuracy: 0.9085\n",
      "Train loss: 0.06662, train accuracy: 0.98562\n",
      "Test loss: 0.29333, test accuracy: 0.905\n",
      "Train loss: 0.07477, train accuracy: 0.95425\n",
      "Test loss: 0.22687, test accuracy: 0.9045\n",
      "Train loss: 0.07659, train accuracy: 0.98375\n",
      "Test loss: 0.326, test accuracy: 0.9025\n",
      "Train loss: 0.09119, train accuracy: 0.9435\n",
      "Test loss: 0.23848, test accuracy: 0.9045\n",
      "Train loss: 0.09101, train accuracy: 0.97738\n",
      "Test loss: 0.36107, test accuracy: 0.897\n",
      "Train loss: 0.11125, train accuracy: 0.93438\n",
      "Test loss: 0.23916, test accuracy: 0.904\n",
      "Train loss: 0.09158, train accuracy: 0.977\n",
      "Test loss: 0.31291, test accuracy: 0.9035\n",
      "Train loss: 0.08208, train accuracy: 0.94888\n",
      "Test loss: 0.2175, test accuracy: 0.9135\n",
      "Train loss: 0.05241, train accuracy: 0.988\n",
      "Test loss: 0.2175, test accuracy: 0.9125\n",
      "Train loss: 0.05135, train accuracy: 0.988\n",
      "Test loss: 0.29735, test accuracy: 0.9055\n",
      "Train loss: 0.07178, train accuracy: 0.95438\n",
      "Test loss: 0.2262, test accuracy: 0.9045\n",
      "Train loss: 0.07223, train accuracy: 0.986\n",
      "Test loss: 0.29296, test accuracy: 0.906\n",
      "Train loss: 0.06783, train accuracy: 0.957\n",
      "Test loss: 0.21723, test accuracy: 0.914\n",
      "Train loss: 0.04909, train accuracy: 0.98938\n",
      "Test loss: 0.22138, test accuracy: 0.918\n",
      "Train loss: 0.04438, train accuracy: 0.98862\n",
      "Test loss: 0.26639, test accuracy: 0.9125\n",
      "Train loss: 0.05383, train accuracy: 0.9685\n",
      "Test loss: 0.22036, test accuracy: 0.9085\n",
      "Train loss: 0.05932, train accuracy: 0.98975\n",
      "Test loss: 0.28622, test accuracy: 0.91\n",
      "Train loss: 0.06115, train accuracy: 0.96188\n",
      "Test loss: 0.21744, test accuracy: 0.913\n",
      "Train loss: 0.05065, train accuracy: 0.9915\n",
      "Test loss: 0.2392, test accuracy: 0.9155\n",
      "Train loss: 0.04232, train accuracy: 0.98238\n",
      "Test loss: 0.23509, test accuracy: 0.9175\n",
      "Train loss: 0.04094, train accuracy: 0.98475\n",
      "Test loss: 0.21758, test accuracy: 0.912\n",
      "Train loss: 0.04564, train accuracy: 0.99175\n",
      "Test loss: 0.2699, test accuracy: 0.9135\n",
      "Train loss: 0.05042, train accuracy: 0.9695\n",
      "Test loss: 0.21845, test accuracy: 0.9105\n",
      "Train loss: 0.04818, train accuracy: 0.99225\n",
      "Test loss: 0.25634, test accuracy: 0.9165\n",
      "Train loss: 0.04391, train accuracy: 0.97712\n",
      "Test loss: 0.22248, test accuracy: 0.9165\n",
      "Train loss: 0.03841, train accuracy: 0.99162\n",
      "Test loss: 0.22521, test accuracy: 0.9185\n",
      "Train loss: 0.03713, train accuracy: 0.99088\n",
      "Test loss: 0.24922, test accuracy: 0.9165\n",
      "Train loss: 0.03965, train accuracy: 0.98188\n",
      "Test loss: 0.21957, test accuracy: 0.914\n",
      "Train loss: 0.04195, train accuracy: 0.99362\n",
      "Test loss: 0.26476, test accuracy: 0.9155\n",
      "Train loss: 0.04327, train accuracy: 0.976\n",
      "Test loss: 0.22031, test accuracy: 0.9145\n",
      "Train loss: 0.04134, train accuracy: 0.99375\n",
      "Test loss: 0.25595, test accuracy: 0.9175\n",
      "Train loss: 0.03881, train accuracy: 0.98088\n",
      "Test loss: 0.22374, test accuracy: 0.914\n",
      "Train loss: 0.03523, train accuracy: 0.99288\n",
      "Test loss: 0.23464, test accuracy: 0.9175\n",
      "Train loss: 0.03305, train accuracy: 0.99025\n",
      "Test loss: 0.23932, test accuracy: 0.9185\n",
      "Train loss: 0.0329, train accuracy: 0.98975\n",
      "Test loss: 0.22438, test accuracy: 0.9135\n",
      "Train loss: 0.03399, train accuracy: 0.99425\n",
      "Test loss: 0.25654, test accuracy: 0.9165\n",
      "Train loss: 0.03546, train accuracy: 0.98338\n",
      "Test loss: 0.22328, test accuracy: 0.9145\n",
      "Train loss: 0.03651, train accuracy: 0.99588\n",
      "Test loss: 0.26974, test accuracy: 0.916\n",
      "Train loss: 0.03822, train accuracy: 0.97925\n",
      "Test loss: 0.22474, test accuracy: 0.9135\n",
      "Train loss: 0.03993, train accuracy: 0.99488\n",
      "Test loss: 0.2905, test accuracy: 0.914\n",
      "Train loss: 0.04454, train accuracy: 0.971\n",
      "Test loss: 0.2321, test accuracy: 0.9095\n",
      "Train loss: 0.05098, train accuracy: 0.99225\n",
      "Test loss: 0.35659, test accuracy: 0.905\n",
      "Train loss: 0.07498, train accuracy: 0.95062\n",
      "Test loss: 0.31081, test accuracy: 0.8845\n",
      "Train loss: 0.13755, train accuracy: 0.95138\n",
      "Test loss: 0.92562, test accuracy: 0.8745\n",
      "Train loss: 0.56591, train accuracy: 0.87875\n",
      "Test loss: 0.56584, test accuracy: 0.814\n",
      "Train loss: 0.42483, train accuracy: 0.84438\n",
      "Test loss: 1.04456, test accuracy: 0.869\n",
      "Train loss: 0.71566, train accuracy: 0.87312\n",
      "Test loss: 0.56811, test accuracy: 0.883\n",
      "Train loss: 0.27184, train accuracy: 0.89512\n",
      "Test loss: 1.21467, test accuracy: 0.6215\n",
      "Train loss: 1.14042, train accuracy: 0.63225\n",
      "Test loss: 0.51407, test accuracy: 0.877\n",
      "Train loss: 0.26789, train accuracy: 0.8885\n",
      "Test loss: 0.92067, test accuracy: 0.8675\n",
      "Train loss: 0.66608, train accuracy: 0.86975\n",
      "Test loss: 0.4512, test accuracy: 0.8775\n",
      "Train loss: 0.24215, train accuracy: 0.88675\n",
      "Test loss: 0.48239, test accuracy: 0.8075\n",
      "Train loss: 0.40244, train accuracy: 0.8375\n",
      "Test loss: 0.22946, test accuracy: 0.902\n",
      "Train loss: 0.13131, train accuracy: 0.97575\n",
      "Test loss: 0.43332, test accuracy: 0.874\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = [], []\n",
    "train_acc, test_acc = [], []\n",
    "\n",
    "d.train()\n",
    "for t in range(N_EPOCHS):\n",
    "    y_pred = d(X_train_tensor)\n",
    "    loss = loss_fn(y_pred.squeeze(), y_train_tensor.float())\n",
    "    acc = ((y_pred>0.5).squeeze() == y_train_tensor).numpy().mean()\n",
    "    train_loss.append(float(loss))\n",
    "    train_acc.append(acc)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    y_pred = d(X_test_tensor)\n",
    "    loss = loss_fn(y_pred.squeeze(), y_test_tensor.float())\n",
    "    acc = ((y_pred>0.5).squeeze() == y_test_tensor).numpy().mean()\n",
    "    test_loss.append(float(loss))\n",
    "    test_acc.append(acc)\n",
    "    \n",
    "    print(f\"Train loss: {round(train_loss[-1],5)}, train accuracy: {round(train_acc[-1],5)}\")\n",
    "    print(f\"Test loss: {round(test_loss[-1],5)}, test accuracy: {round(test_acc[-1],5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3e0f24ef-d788-440a-9b00-32f8d35ffd0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3720, 0.0813, 0.4079,  ..., 0.3477, 0.4968, 0.2155],\n",
       "        [0.9243, 0.3137, 0.3303,  ..., 0.7141, 0.2330, 0.1494],\n",
       "        [0.5804, 0.2009, 0.3148,  ..., 0.6338, 0.4188, 0.3704],\n",
       "        ...,\n",
       "        [0.2821, 0.0024, 0.8719,  ..., 0.8228, 0.2355, 0.4027],\n",
       "        [0.2776, 0.8033, 0.4498,  ..., 0.4598, 0.1108, 0.4704],\n",
       "        [0.1702, 0.5336, 0.6603,  ..., 0.4552, 0.1610, 0.3729]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13eabb98-e35d-4775-86e0-708c0ac51234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43md\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mDeepNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/repos/playground/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "078a23fb-1188-4bcd-97fc-ea2ab2276506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv0): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (relu0): ReLU()\n",
       "  (conv1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2ded4-bca9-4c0c-8d12-b0df6530eee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
