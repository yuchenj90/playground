{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d786ba2-f798-4d77-b0e1-63f80f456ef6",
   "metadata": {},
   "source": [
    "### write code to implement backward propagation for cross-entropy loss, only need to support the following type of layers:\n",
    "- Linear layer with in_dim and out_dim\n",
    "- ReLU layer\n",
    "- Softmax layer\n",
    "\n",
    "Example: \n",
    "``` Python\n",
    "Input:\n",
    "    n_labels = 10\n",
    "    layers = ['relu', 'relu', 'softmax']\n",
    "    weights = [np.random.uniform(-0.01, 0.01, (128,64)), np.random.uniform(-0.01, 0.01, (64,16)), np.random.uniform(-0.01, 0.01, (16,10))]\n",
    "    \n",
    "    batch_size = 256\n",
    "    X_train = np.random.rand(batch_size,128)\n",
    "    y_train = np.random.randint(low=0,high=n_labels,size=batch_size)\n",
    "\n",
    "Output:\n",
    "    w_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc5a9a4b-a6ff-4040-bbfe-c2cbc0c6217b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DNN:\n",
    "    def __init__(self, n_labels, layers, weights):\n",
    "        assert len(layers) == len(weights), \"Every linear layer must followed by a non-linear activation!\"\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_labels = n_labels\n",
    "        self.layers = layers\n",
    "        self.weights = weights\n",
    "        self.eps = 1e-100\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.forward_values = []\n",
    "        for i in range(self.n_layers):\n",
    "            w = self.weights[i]\n",
    "            d = {'input': x}\n",
    "            x = x @ w\n",
    "            d['hidden'] = x\n",
    "            if self.layers[i] == 'relu':\n",
    "                x = np.maximum(x,0)\n",
    "            elif self.layers[i] == 'softmax':\n",
    "                # x = (np.exp(x).transpose()/np.exp(x).sum(axis=1)).transpose() # can cause overflow when x is large\n",
    "                x_max = x.max(axis=1).reshape(-1,1)\n",
    "                x = (np.exp(x-x_max).transpose()/np.exp(x-x_max).sum(axis=1)).transpose()\n",
    "            d['output'] = x\n",
    "            self.forward_values.append(d)\n",
    "        return x\n",
    "    \n",
    "    def CrossEntropyLoss(self, y_pred, y_true):\n",
    "        return -np.array([np.log(np.maximum(y_pred[i,y_true[i]], self.eps)) for i in range(len(y_true))]).mean()\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        self.grads = []\n",
    "        for m in range(self.n_layers-1, -1, -1):\n",
    "            d = {}\n",
    "            if len(self.grads)==0:\n",
    "                onehot_y = np.array([np.eye(self.n_labels)[y] for y in y_train])\n",
    "                d['grad_o'] = -onehot_y/(self.forward_values[-1]['output']+self.eps) # (B, n_labels)\n",
    "            else:\n",
    "                d['grad_o'] = self.grads[-1]['grad_h'] @ self.weights[m+1].transpose()  # (B, d_h^m) \n",
    "            if self.layers[m] == 'relu':\n",
    "                d['grad_h'] = (self.forward_values[m]['hidden']>0) * d['grad_o'] # (B, d_h^(m))\n",
    "            elif self.layers[m] == 'softmax':    \n",
    "                do_h = np.array([np.diag(row) for row in self.forward_values[m]['output']]) # (B, d_h^m, d_h^m)\n",
    "                do_h = do_h - self.forward_values[m]['output'][:,:,np.newaxis] * self.forward_values[m]['output'][:,np.newaxis,:] # (B, d_h^m, d_h^m)\n",
    "                d['grad_h'] = (d['grad_o'][:,np.newaxis,:] * do_h).sum(axis=-1) # (B, d_h^m)\n",
    "\n",
    "            d['grad_w'] = (self.forward_values[m]['input'][:,:,np.newaxis] * d['grad_h'][:,np.newaxis,:]).sum(axis=0)\n",
    "            # print(d['grad_w'].shape) # (d_h^(m-1), d_h^m)\n",
    "            self.grads.append(d)\n",
    "        return [self.grads[i]['grad_w'] for i in range(len(self.grads)-1,-1,-1)] \n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e972669-d5c3-426f-a082-ef82dfa2c5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_labels = 10\n",
    "layers = ['relu', 'relu', 'softmax']\n",
    "weights = [np.random.uniform(-0.01, 0.01, (128,64)), np.random.uniform(-0.01, 0.01, (64,16)), np.random.uniform(-0.01, 0.01, (16,10))]\n",
    "\n",
    "model = DNN(n_labels, layers, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11d6656b-882d-4d09-b10b-9395629432e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "X_train = np.random.rand(batch_size,128)\n",
    "y_train = np.random.randint(low=0,high=2,size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3857fbe1-870d-4f07-996d-11825058776d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, X_train, y_train, n_iter:int=100, lr: float=1e-3):\n",
    "    for i in range(n_iter):\n",
    "        x = model.forward(X_train)\n",
    "        loss = model.CrossEntropyLoss(x, y_train)\n",
    "        w_grad = model.backward(y_train)\n",
    "        for k in range(model.n_layers):\n",
    "            model.weights[k] -= lr * w_grad[k]\n",
    "        print(f'Iteration {i+1}: loss={loss}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5811147f-b103-4309-91bb-7e66a5b7275a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: loss=2.302581840761669\n",
      "Iteration 2: loss=2.3025787061484313\n",
      "Iteration 3: loss=2.3025754974747974\n",
      "Iteration 4: loss=2.3025721906269268\n",
      "Iteration 5: loss=2.3025687467767293\n",
      "Iteration 6: loss=2.3025651240496594\n",
      "Iteration 7: loss=2.3025612786530134\n",
      "Iteration 8: loss=2.302557176646829\n",
      "Iteration 9: loss=2.3025527854932877\n",
      "Iteration 10: loss=2.3025480291738605\n",
      "Iteration 11: loss=2.3025428421467056\n",
      "Iteration 12: loss=2.302537157954361\n",
      "Iteration 13: loss=2.3025309004751886\n",
      "Iteration 14: loss=2.3025239853953927\n",
      "Iteration 15: loss=2.3025163277446827\n",
      "Iteration 16: loss=2.302507832384738\n",
      "Iteration 17: loss=2.302498360992307\n",
      "Iteration 18: loss=2.3024877817475535\n",
      "Iteration 19: loss=2.3024759172859692\n",
      "Iteration 20: loss=2.3024625643592636\n",
      "Iteration 21: loss=2.3024474539352866\n",
      "Iteration 22: loss=2.302430328591459\n",
      "Iteration 23: loss=2.3024108275054878\n",
      "Iteration 24: loss=2.3023885425380937\n",
      "Iteration 25: loss=2.3023629628858897\n",
      "Iteration 26: loss=2.3023334339547197\n",
      "Iteration 27: loss=2.3022991525442578\n",
      "Iteration 28: loss=2.302259160397164\n",
      "Iteration 29: loss=2.302212216038906\n",
      "Iteration 30: loss=2.302156728201105\n",
      "Iteration 31: loss=2.302090708883416\n",
      "Iteration 32: loss=2.3020115837950446\n",
      "Iteration 33: loss=2.3019159843162704\n",
      "Iteration 34: loss=2.3017993760823106\n",
      "Iteration 35: loss=2.3016557685621106\n",
      "Iteration 36: loss=2.301476932298886\n",
      "Iteration 37: loss=2.301251565242265\n",
      "Iteration 38: loss=2.3009637041007087\n",
      "Iteration 39: loss=2.3005903725685504\n",
      "Iteration 40: loss=2.3000980209266837\n",
      "Iteration 41: loss=2.2994360296334886\n",
      "Iteration 42: loss=2.298526305603036\n",
      "Iteration 43: loss=2.2972444332393813\n",
      "Iteration 44: loss=2.295384544696981\n",
      "Iteration 45: loss=2.2925916958322983\n",
      "Iteration 46: loss=2.288223433890318\n",
      "Iteration 47: loss=2.2810472282918863\n",
      "Iteration 48: loss=2.2685310760119375\n",
      "Iteration 49: loss=2.24503556314397\n",
      "Iteration 50: loss=2.1967463718118134\n",
      "Iteration 51: loss=2.086215744089158\n",
      "Iteration 52: loss=1.8067483959257893\n",
      "Iteration 53: loss=1.174803183069704\n",
      "Iteration 54: loss=0.7450424093156067\n",
      "Iteration 55: loss=0.7176237995147883\n",
      "Iteration 56: loss=0.7090120651481866\n",
      "Iteration 57: loss=0.7059230782901085\n",
      "Iteration 58: loss=0.7326852676962231\n",
      "Iteration 59: loss=1.3423202391385693\n",
      "Iteration 60: loss=2.481149021313322\n",
      "Iteration 61: loss=2.1599474082313277\n",
      "Iteration 62: loss=1.7545880145119936\n",
      "Iteration 63: loss=0.903087685412645\n",
      "Iteration 64: loss=0.7222201801775587\n",
      "Iteration 65: loss=0.7111236713684448\n",
      "Iteration 66: loss=0.7100759348826031\n",
      "Iteration 67: loss=0.7329863264388446\n",
      "Iteration 68: loss=0.8924528022516689\n",
      "Iteration 69: loss=1.284288836228729\n",
      "Iteration 70: loss=1.0289592649873898\n",
      "Iteration 71: loss=0.807811897441856\n",
      "Iteration 72: loss=0.7424933429065517\n",
      "Iteration 73: loss=0.722405390098697\n",
      "Iteration 74: loss=0.7206481173946424\n",
      "Iteration 75: loss=0.7335219388404461\n",
      "Iteration 76: loss=0.7749146216468202\n",
      "Iteration 77: loss=0.8497781787604726\n",
      "Iteration 78: loss=0.8625811506167866\n",
      "Iteration 79: loss=0.7801684586568678\n",
      "Iteration 80: loss=0.7262359669760136\n",
      "Iteration 81: loss=0.712034810114703\n",
      "Iteration 82: loss=0.7077156972004878\n",
      "Iteration 83: loss=0.7078859791947356\n",
      "Iteration 84: loss=0.71445340852537\n",
      "Iteration 85: loss=0.736537155222874\n",
      "Iteration 86: loss=0.791320865993766\n",
      "Iteration 87: loss=0.8557335325695408\n",
      "Iteration 88: loss=0.8183280241988772\n",
      "Iteration 89: loss=0.7438817281154129\n",
      "Iteration 90: loss=0.7148872859086741\n",
      "Iteration 91: loss=0.7069328786950259\n",
      "Iteration 92: loss=0.703694994177506\n",
      "Iteration 93: loss=0.7025711756858081\n",
      "Iteration 94: loss=0.7035579727337361\n",
      "Iteration 95: loss=0.7086770641435594\n",
      "Iteration 96: loss=0.7241251989275316\n",
      "Iteration 97: loss=0.7622237417305302\n",
      "Iteration 98: loss=0.8192432205654072\n",
      "Iteration 99: loss=0.8182890519057581\n",
      "Iteration 100: loss=0.7531525355957939\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37cfdb04-ed53-47eb-b2de-dfd861f99324",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54589656, 0.44249833, 0.00145189, ..., 0.00143438, 0.00144069,\n",
       "        0.00146423],\n",
       "       [0.53285652, 0.44339755, 0.00297046, ..., 0.00293908, 0.00295051,\n",
       "        0.00299249],\n",
       "       [0.53848808, 0.44352941, 0.0022496 , ..., 0.00222453, 0.00223358,\n",
       "        0.00226723],\n",
       "       ...,\n",
       "       [0.53349522, 0.443448  , 0.00288425, ..., 0.00285364, 0.00286474,\n",
       "        0.00290583],\n",
       "       [0.53970658, 0.44345963, 0.00210591, ..., 0.00208212, 0.00209078,\n",
       "        0.00212262],\n",
       "       [0.54325247, 0.44303766, 0.00171515, ..., 0.00169506, 0.00170236,\n",
       "        0.0017293 ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d6bc6f2-ed64-4dc8-81a2-f8c0235a3fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49609375, 0.50390625])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts/counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1dce8d-d2a9-42c3-a483-22ffeb1e74d9",
   "metadata": {},
   "source": [
    "# test on real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "843adcf5-78e2-4143-9f64-5617273e510c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset from torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data',\n",
    "                                           train = True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data',\n",
    "                                          train = False,\n",
    "                                          transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e0d25ac2-d632-4c3a-b942-f096a3c1e7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = train_dataset.data.numpy().reshape(train_dataset.data.shape[0],-1)\n",
    "y_train = train_dataset.targets.numpy()\n",
    "\n",
    "x_test = test_dataset.data.numpy().reshape(test_dataset.data.shape[0],-1)\n",
    "y_test = test_dataset.targets.numpy()\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f6562123-a540-499d-8949-10e93c0b6b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = np.isin(y_train, [0,1,2])\n",
    "x_train = x_train[idx][:10000]\n",
    "y_train = y_train[idx][:10000]\n",
    "\n",
    "idx = np.isin(y_test, [0,1,2])\n",
    "x_test = x_test[idx]\n",
    "y_test = y_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4268789a-68ad-4fd1-99a0-7bee04d6e3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_labels = 10\n",
    "layers = ['relu', 'relu', 'softmax']\n",
    "weights = [np.random.uniform(-0.01, 0.01, (784,64)), np.random.uniform(-0.01, 0.01, (64,16)), np.random.uniform(-0.01, 0.01, (16,10))]\n",
    "\n",
    "model = DNN(n_labels, layers, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "be9e90a8-feb3-47df-abfd-cc8225e094bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3162, 0.3677, 0.3161])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts/counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "536c65b2-5049-4068-9cbf-e477d290f0db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: loss=2.3081007216388674\n",
      "Iteration 2: loss=2.304129284785334\n",
      "Iteration 3: loss=2.30225942529753\n",
      "Iteration 4: loss=2.3006393262618205\n",
      "Iteration 5: loss=2.2985152648523117\n",
      "Iteration 6: loss=2.2951000184156043\n",
      "Iteration 7: loss=2.289144780193859\n",
      "Iteration 8: loss=2.278128286986896\n",
      "Iteration 9: loss=2.2561096159487106\n",
      "Iteration 10: loss=2.2075770729605986\n",
      "Iteration 11: loss=2.0863484133715007\n",
      "Iteration 12: loss=1.7616432684962071\n",
      "Iteration 13: loss=1.2766035064456356\n",
      "Iteration 14: loss=1.0882893412952794\n",
      "Iteration 15: loss=1.642467203049152\n",
      "Iteration 16: loss=3.2795592205798\n",
      "Iteration 17: loss=2.1605288901326456\n",
      "Iteration 18: loss=1.8394422767011216\n",
      "Iteration 19: loss=1.4205901399187493\n",
      "Iteration 20: loss=1.32398129036869\n",
      "Iteration 21: loss=1.269218126053006\n",
      "Iteration 22: loss=1.270931587123667\n",
      "Iteration 23: loss=1.3558513595382726\n",
      "Iteration 24: loss=1.5775408947311302\n",
      "Iteration 25: loss=1.2980087176647788\n",
      "Iteration 26: loss=1.111015955779886\n",
      "Iteration 27: loss=0.6827716932298199\n",
      "Iteration 28: loss=0.6593800429011558\n",
      "Iteration 29: loss=0.9288794656786711\n",
      "Iteration 30: loss=2.2681332405709558\n",
      "Iteration 31: loss=1.4230821096875816\n",
      "Iteration 32: loss=1.0983525986251355\n",
      "Iteration 33: loss=0.9652921406721449\n",
      "Iteration 34: loss=0.7951919363042951\n",
      "Iteration 35: loss=0.7106992598589579\n",
      "Iteration 36: loss=0.6186493052915374\n",
      "Iteration 37: loss=0.43795747846117694\n",
      "Iteration 38: loss=0.2545126902546809\n",
      "Iteration 39: loss=0.19533655280942194\n",
      "Iteration 40: loss=0.1648607168579156\n",
      "Iteration 41: loss=0.14616978562595165\n",
      "Iteration 42: loss=0.13426330869531408\n",
      "Iteration 43: loss=0.12399305795690131\n",
      "Iteration 44: loss=0.11639385225067704\n",
      "Iteration 45: loss=0.10892357328410744\n",
      "Iteration 46: loss=0.10328727997736191\n",
      "Iteration 47: loss=0.09843570322792296\n",
      "Iteration 48: loss=0.09471810458871237\n",
      "Iteration 49: loss=0.09164052387075063\n",
      "Iteration 50: loss=0.08907170300968233\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, x_train, y_train, lr=1e-6, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5b95a31b-35f4-499e-b16a-b8c6b73da75d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9825230378137909"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(x_test).argmax(axis=1) == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "22dd3c31-eb93-4b92-933d-6114ccab19cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 0, 1, 2])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5e85d-a7e4-487e-8007-fea7697f8c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385cc89-6240-4887-a8f2-1ac994fdb414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
