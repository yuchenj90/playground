{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Batch normalization normalizes across all data points in the batch, while layer normalization normalizes on a single data point across the hidden units.\n",
    "\n",
    "> Why transformer/LLM uses layernorm rather than batchnorm?\n",
    "> - Batch stats vary with the time-layer of the neural network and they need to be maintained for different timestamps\n",
    "> - The number of layers is input-dependent, so in some mini-batches stats may not be available.\n",
    ">- Batch normalization requires all data in the batch to compute, which could be an issue when perform efficient training such as data parallelism on different GPUs (require sync for the entire batch at each BN node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, n_features:int, eps=1e-5, trainable=False):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "        self.beta = nn.Parameter(torch.zeros(self.n_features))\n",
    "        self.gamma = nn.Parameter(torch.ones(self.n_features))\n",
    "        if not self.trainable:\n",
    "            self.beta.requires_grad = False\n",
    "            self.gamma.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x has shape (B, n_features)\n",
    "        mu = x.mean(axis=0)\n",
    "        sigma = ((x-mu)**2).mean(axis=0)+self.eps\n",
    "        x = (x - mu)/torch.sqrt(sigma) \n",
    "        x = self.gamma*x + self.beta\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9681,  1.2817,  0.2988,  1.0407,  1.4796,  1.0418,  1.4976, -1.8626,\n",
      "          1.3309,  0.0239, -1.7359,  0.5197],\n",
      "        [ 1.1636, -0.7685,  1.0525,  0.9157, -1.0946, -0.4234,  0.7327, -1.9050,\n",
      "         -1.0799,  1.5753, -1.3696, -0.6924],\n",
      "        [-0.1481, -0.6816,  1.1876, -0.8209,  1.1980, -0.8677, -1.4088,  0.9827,\n",
      "          0.9965, -0.9997,  0.1709,  0.0177],\n",
      "        [-1.8566, -0.3192,  1.1693, -0.1286,  1.1690, -0.2256,  0.5283, -0.1031,\n",
      "         -0.1562, -0.6333, -1.5097,  0.2571],\n",
      "        [ 0.8689, -1.0719, -0.7818,  2.3533, -0.0615,  0.6098,  0.4189, -0.2295,\n",
      "         -1.2368, -1.2044,  0.8641,  1.5173],\n",
      "        [-1.4383, -1.4673,  1.4675, -1.0227, -1.1297,  1.4058, -0.7517,  0.1495,\n",
      "         -1.1174,  1.2494,  1.0894,  0.4233],\n",
      "        [-0.4150,  0.8856, -0.2005,  0.3491, -0.8582,  1.5819, -0.5425,  1.2694,\n",
      "          0.9575, -0.8455, -1.1827, -1.5559],\n",
      "        [ 0.7443, -1.4563, -0.1399,  1.7998, -0.3497, -1.6391,  1.5039,  1.3012,\n",
      "          0.9346,  0.0249, -0.5778,  1.4288],\n",
      "        [ 0.8668,  1.0631, -1.1913,  0.0354,  0.0411, -0.6812, -1.0632,  1.2639,\n",
      "         -0.6140,  1.4250,  0.9672,  0.5752],\n",
      "        [-1.3306,  0.7600,  1.2886, -0.1702,  0.0988,  1.6003, -1.2688,  1.0699,\n",
      "         -0.3113, -0.3681,  0.0389, -0.1781],\n",
      "        [ 0.9631,  0.7163, -0.8681, -0.8319,  0.5028, -0.2068,  1.1264, -0.4292,\n",
      "         -0.6468, -0.9974,  0.5795,  0.2175],\n",
      "        [ 1.3380, -1.3933, -0.7298, -1.1121, -1.1384, -1.6352, -0.1959, -0.6194,\n",
      "          0.5734,  0.9932,  1.3974, -1.3473],\n",
      "        [ 0.0761, -0.2361, -1.4376, -0.5485,  0.9941, -0.3245, -0.8521, -0.7519,\n",
      "          1.5462, -0.2407,  0.3728, -1.0723],\n",
      "        [-0.9175,  0.4101, -1.3203, -0.7275, -0.8121,  0.3086, -1.2654,  0.6079,\n",
      "          1.1121,  0.1467,  1.1123, -0.0420],\n",
      "        [ 0.6785,  1.0571, -0.6060, -0.4852, -1.5134, -0.6621,  0.9921, -0.4821,\n",
      "         -1.0799,  1.3663,  0.3973, -1.5933],\n",
      "        [ 0.3749,  1.2202,  0.8109, -0.6465,  1.4743,  0.1176,  0.5484, -0.2617,\n",
      "         -1.2090, -1.5156, -0.6140,  1.5248]])\n",
      "tensor([-4.0978e-08, -1.1921e-07, -1.3411e-07,  5.2154e-08,  2.2352e-08,\n",
      "         2.9802e-08,  4.4703e-08,  7.4506e-08,  2.2352e-08, -1.3411e-07,\n",
      "         6.7055e-08, -7.4506e-09])\n",
      "tensor([1.0327, 1.0328, 1.0328, 1.0328, 1.0328, 1.0328, 1.0328, 1.0328, 1.0328,\n",
      "        1.0328, 1.0328, 1.0328])\n"
     ]
    }
   ],
   "source": [
    "bn = BatchNorm(12)\n",
    "x = torch.rand((16,12))*torch.arange(1,13)\n",
    "print(bn(x))\n",
    "print(bn(x).mean(axis=0))\n",
    "print(bn(x).std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 3, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,12)+(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: Tuple[int], trainable: bool=False, eps: float=1e-7):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.trainable = trainable\n",
    "        self.eps = eps\n",
    "        self.beta = nn.Parameter(torch.zeros(self.normalized_shape))\n",
    "        self.gamma = nn.Parameter(torch.ones(self.normalized_shape))\n",
    "        if not self.trainable:\n",
    "            self.beta.requires_grad = False\n",
    "            self.gamma.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d = len(self.normalized_shape)\n",
    "        assert tuple(x.shape[-d:]) == self.normalized_shape, f\"Expected the last dimensions of input shape match normalized shape {self.normalized_shape}, got {tuple(x.shape[-d:])} instead.\"\n",
    "        \n",
    "        t = tuple([-i for i in range(1,d+1)])\n",
    "        mu = x.mean(t)\n",
    "        sigma = (x**2).mean(t) - mu**2 + self.eps\n",
    "        \n",
    "        for i in range(d):\n",
    "            mu = torch.stack(tuple([mu for _ in range(self.normalized_shape[i])]), axis=-1)\n",
    "            sigma = torch.stack(tuple([sigma for _ in range(self.normalized_shape[i])]), axis=-1)\n",
    "        \n",
    "        x = (x - mu) / torch.sqrt(sigma)\n",
    "        x = self.gamma * x + self.beta\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from typing import List\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_word_embeddings(sentences: List[str]):\n",
    "\n",
    "    # Tokenize and encode text using batch_encode_plus\n",
    "    # The function returns a dictionary containing the token IDs and attention masks\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        sentences,                    # List of input texts\n",
    "        padding=True,              # Pad to the maximum sequence length\n",
    "        truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "        return_tensors='pt',      # Return PyTorch tensors\n",
    "        add_special_tokens=True    # Add special tokens CLS and SEP\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids']  # Token IDs\n",
    "    # print input IDs\n",
    "    print(f\"Input ID: {input_ids}\")\n",
    "    attention_mask = encoding['attention_mask']  # Attention mask\n",
    "    # print attention mask\n",
    "    print(f\"Attention mask: {attention_mask}\")\n",
    "\n",
    "    # Generate embeddings using BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        word_embeddings = outputs.last_hidden_state  # This contains the embeddings\n",
    "\n",
    "    # Output the shape of word embeddings\n",
    "    print(f\"Shape of Word Embeddings: {word_embeddings.shape}\")\n",
    "    return attention_mask, word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID: tensor([[  101,  1045,  2052,  2066,  2000,  5271,  2978,  3597,  2378,   102,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2651,  2003,  1037,  3376,  2154,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  3577,  2038,  2074,  2180,  1996, 16798,  2549,  9944,  2452,\n",
      "           999,   102,     0,     0],\n",
      "        [  101,  4365,  1010,  1045,  2572,  2397,  2005,  1996,  3460,  1005,\n",
      "          1055,  6098,   999,   102],\n",
      "        [  101,  2017,  2074,  2562,  2006,  2667,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2994, 13219,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Shape of Word Embeddings: torch.Size([6, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I would like to sell bitcoin\", \"Today is a beautiful day.\", \"Spain has just won the 2024 Euro cup!\", \"Damn, I am late for the doctor's appointment!\", \"You just keep on trying\", \"Stay foolish\"]\n",
    "\n",
    "att_mask, word_embeddings = get_word_embeddings(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ln = LayerNorm((14,768))\n",
    "x = ln(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 14, 768])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.9828e-10,  8.3376e-09,  5.6766e-09,  7.0958e-10, -1.4192e-09,\n",
       "        -9.9341e-09])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean((-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.var((-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
